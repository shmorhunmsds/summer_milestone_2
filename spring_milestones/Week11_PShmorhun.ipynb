{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb21f458-c2b3-44f2-be7d-67e95bbbc4f8",
   "metadata": {},
   "source": [
    "# Week 11 - Introduction to Modeling, part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3210712e-ad65-41b1-8337-18250e10f737",
   "metadata": {},
   "source": [
    "# 1. Lesson - No lesson this week"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfdcf3e4-28cc-401d-b364-5ea3dac4f164",
   "metadata": {},
   "source": [
    "# 2. Weekly graph question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5602bd5e-4123-454e-9aeb-94857918d6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740d7e7a-d244-4cbb-9804-97583898ab85",
   "metadata": {},
   "source": [
    "The book names one of Vonnegut's rules as \"keep it simple\" and another as \"have the guts to cut.\"  Here is some data from the previous week's lesson.  If you had to cut one of the two plots below, which would it be?  Which seems more interesting or important?  Explain.  (Should \"amount of training data used\" or \"number of estimators\" be on the x-axis.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d26777f-60b8-4e82-9109-49b7b89b0d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "num_points = 10000\n",
    "feature_1a = np.random.random(size = num_points) * 3\n",
    "feature_2a = np.random.random(size = num_points) * 3\n",
    "feature_3a = np.random.random(size = num_points) * 3\n",
    "train_target = (feature_1a - 2 * feature_2a) * feature_3a + np.random.normal(size = num_points)\n",
    "feature_1b = np.random.random(size = num_points) * 3\n",
    "feature_2b = np.random.random(size = num_points) * 3\n",
    "feature_3b = np.random.random(size = num_points) * 3\n",
    "test_target = (feature_1b - 2 * feature_2b) * feature_3b + np.random.normal(size = num_points)\n",
    "train_df = pd.DataFrame({\"f1\": feature_1a, \"f2\": feature_2a, \"f3\": feature_3a})\n",
    "test_df = pd.DataFrame({\"f1\": feature_1b, \"f2\": feature_2b, \"f3\": feature_3b})\n",
    "rf = RandomForestRegressor()\n",
    "rf.fit(train_df.values, train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3043eff6-7f5c-43e9-bad1-727340c7be4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_lst = list()\n",
    "rf = RandomForestRegressor()\n",
    "for x in range(round(num_points / 20), num_points, round(num_points / 20)):\n",
    "    rf.fit(train_df.values[0:x,:], train_target[0:x])\n",
    "    rmse_lst.append(root_mean_squared_error(rf.predict(test_df.values), test_target))\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "430dff57-a039-4782-be1f-d0db87b4f5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(rmse_lst)\n",
    "plt.xlabel(\"Amount of training data used (20 = max)\")\n",
    "plt.ylabel(\"Loss function\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa974208-3f1a-4536-bc20-695e0ff844c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_trees_lst = list()\n",
    "for n_estimators in range(1, 100, 3):\n",
    "    rf = RandomForestRegressor(n_estimators = n_estimators)\n",
    "    rf.fit(train_df.values, train_target)\n",
    "    num_trees_lst.append(root_mean_squared_error(rf.predict(test_df.values), test_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb48cf7f-bfc3-4c8a-ba7b-25ea9d9167c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(num_trees_lst)\n",
    "plt.xlabel(\"Number of estimators (trees)\")\n",
    "plt.ylabel(\"Loss function\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73c9b5c",
   "metadata": {},
   "source": [
    "### If you had to cut one of the two plots below, which would it be?  Which seems more interesting or important?  Explain. (Should \"amount of training data used\" or \"number of estimators\" be on the x-axis.)\n",
    "- The first plot shows RMSE as the size of the training set increases, with the RMSE dropping as the size of the training data is increased. As more data is added, the model \"learns\" to build a better representation of the underlying data and generalizes better, lowering the error. \n",
    "- The second plot shows the RMSE as the number of estimators in the decision tree model is increased, with the RMSE dropping as the complexity of the model is increased. As more trees are added, the model benefits from the ensemble effect, which will result in improved model as the model averages across many trees, improving accuracy and lowering error. \n",
    "- If I had to only pick one model, I'd go with the first plot because the learning curve (amount of training data used) offers a more fundamental insight into how the model improves with more data. Thus, having \"amount of training data used\" on the x-axis would be more interesting and important for understanding the overall behavior of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47207b4-5c57-4149-b617-f37589807b82",
   "metadata": {},
   "source": [
    "# 3. Working on your datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845e786d-7afe-4fac-a207-348929442e08",
   "metadata": {},
   "source": [
    "This week, you will do the same types of exercises as last week, but you should use your chosen datasets that someone in your class found last semester. (They likely will not be the particular datasets that you found yourself.)\n",
    "\n",
    "Here are some types of analysis you can do:\n",
    "\n",
    "* Implement a random forest model.\n",
    "* Perform cross-validation.\n",
    "* Tune hyperparameters.\n",
    "* Evaluate a performance metric.\n",
    "\n",
    "If you like, you can try other types of models, too (beyond linear regression and random forest) although you will have many opportunities to do that next semester."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ff453e",
   "metadata": {},
   "source": [
    "## Dataset 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7188f60",
   "metadata": {},
   "source": [
    "### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e695c17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scikit-learn (Machine Learning)\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split, \n",
    "    cross_val_score, \n",
    "    GridSearchCV, \n",
    "    RandomizedSearchCV, \n",
    "    RepeatedKFold\n",
    ")\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, \n",
    "    confusion_matrix, \n",
    "    classification_report\n",
    ")\n",
    "\n",
    "# preprocess data\n",
    "motion_df = pd.read_csv(\"datasets/nfl-playing-surface-analytics/motion_df_encoded.csv\")\n",
    "\n",
    "# Prepare the features (drop leakage columns)\n",
    "X = motion_df.drop(columns=[\n",
    "    'Injury', 'DM_M1', 'DM_M7', 'DM_M28', 'DM_M42'])\n",
    "\n",
    "# One-hot encode categorical columns\n",
    "X = pd.get_dummies(X, dummy_na=True, drop_first=True)\n",
    "\n",
    "# For a cuDF DataFrame with some boolean columns:\n",
    "for col in X.columns:\n",
    "    if X[col].dtype == 'bool':\n",
    "        X[col] = X[col].astype('int32')\n",
    "\n",
    "\n",
    "# Prepare the binary target (0 = no injury, 1 = injury)\n",
    "y = motion_df['Injury'].copy()\n",
    "y_binary = y.copy()\n",
    "y_binary[y_binary > 0] = 1\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, \n",
    "    y_binary, \n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    shuffle=True       \n",
    ")\n",
    "\n",
    "# Initialize and train a RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = rf.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(\"Test Accuracy:\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e99e4f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "# 8. Predict and evaluate\n",
    "y_pred = rf.predict(X_test)\n",
    "y_pred_proba = rf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Classification report\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot()\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show() # great accuracy but very poor recal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cce1f247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. After rf.fit(X_train, y_train):\n",
    "importances = rf.feature_importances_\n",
    "\n",
    "# 2. Turn into a Series for easy sorting & plotting\n",
    "feat_imp = pd.Series(importances, index=X_train.columns)\n",
    "\n",
    "# 3. Sort descending and view the top 10\n",
    "top10 = feat_imp.sort_values(ascending=False).head(10)\n",
    "print(top10)\n",
    "\n",
    "# 4. (Optional) Plot as a horizontal bar chart\n",
    "plt.figure(figsize=(8,5))\n",
    "top10.sort_values().plot.barh()\n",
    "plt.title(\"Top 10 Feature Importances\")\n",
    "plt.xlabel(\"Relative Importance\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eefda7a",
   "metadata": {},
   "source": [
    "### Resampling with SMOTE, then Cross Validation through GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "291dfd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "# Start timing\n",
    "start_time = time.time()\n",
    "\n",
    "# Define the pipeline: first SMOTE, then the classifier.\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('smote', SMOTE(random_state=42)),\n",
    "    ('classifier', RandomForestClassifier(random_state=0))\n",
    "])\n",
    "\n",
    "# Parameter grid for grid search.\n",
    "grid_params = {\n",
    "    'classifier__max_depth': [2, 3, 4, 5, None],\n",
    "    'classifier__min_samples_leaf': [1, 2, 3],\n",
    "    'classifier__min_samples_split': [2, 3, 4],\n",
    "    'classifier__max_features': [2, 3, 4],\n",
    "    'classifier__n_estimators': [75, 100, 125, 150]\n",
    "}\n",
    "\n",
    "# Define the scoring metrics (ensure 'scoring' is defined appropriately).\n",
    "scoring = {\n",
    "    'precision': 'precision',\n",
    "    'recall': 'recall',\n",
    "    'f1': 'f1'\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV.\n",
    "grid_cv = GridSearchCV(pipeline, \n",
    "                       grid_params, \n",
    "                       scoring=scoring, \n",
    "                       cv=3,\n",
    "                       n_jobs=-1, \n",
    "                       refit='f1',\n",
    "                       verbose=3)\n",
    "\n",
    "# Fit on the training data.\n",
    "grid_cv.fit(X_train, y_train)\n",
    "\n",
    "# Print execution time.\n",
    "execution_time = time.time() - start_time\n",
    "print(f\"\\nExecution Time: {execution_time:.2f}s\\n\")\n",
    "\n",
    "# Print the best parameters and best f1 score.\n",
    "print(\"Best Parameters:\")\n",
    "print(grid_cv.best_params_)\n",
    "print(f\"\\nBest F1 Score (CV): {grid_cv.best_score_:.4f}\\n\")\n",
    "\n",
    "# Display all cross-validation results in a sorted DataFrame.\n",
    "cv_results = pd.DataFrame(grid_cv.cv_results_)\n",
    "# Sort by rank_test_f1 (lowest rank is the best)\n",
    "cv_results = cv_results.sort_values('rank_test_f1')\n",
    "print(\"Grid Search CV Results (top 5 rows):\")\n",
    "print(cv_results[['params', 'mean_test_f1', 'std_test_f1', 'rank_test_f1']].head())\n",
    "\n",
    "# If you have a test set, evaluate on it:\n",
    "if 'X_test' in globals() and 'y_test' in globals():\n",
    "    y_pred = grid_cv.predict(X_test)\n",
    "    print(\"\\nClassification Report on Test Set:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    print(\"Confusion Matrix on Test Set:\")\n",
    "    print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad2716e",
   "metadata": {},
   "source": [
    "## Dataset 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18b56a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import os\n",
    "import io\n",
    "import zipfile\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Define the folder path\n",
    "folder_path = 'datasets/nfl-big-data-bowl-2023.zip'\n",
    "extract_path = 'datasets/nfl-big-data-bowl-2023'\n",
    "\n",
    "# Check if the extraction directory already exists\n",
    "if not os.path.exists(extract_path):\n",
    "    # Unzip the file\n",
    "    with zipfile.ZipFile(folder_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_path)\n",
    "    print(f\"Extracted all files to {extract_path}\")\n",
    "else:\n",
    "    print(f\"Directory {extract_path} already exists, skipping extraction.\")\n",
    "\n",
    "# List all CSV files in the folder\n",
    "csv_files = [f for f in os.listdir(extract_path) if f.endswith('.csv')]\n",
    "\n",
    "# Initialize an empty dictionary to store dataframes\n",
    "dataframes = {}\n",
    "\n",
    "# Load each CSV file as a separate dataframe with a progress bar\n",
    "for file in tqdm(csv_files, desc=\"Loading CSV files\", unit=\"file\"):\n",
    "    file_path = os.path.join(extract_path, file)\n",
    "    df_name = os.path.splitext(file)[0]\n",
    "    dataframes[df_name] = pd.read_csv(file_path)\n",
    "\n",
    "# Convert dictionary to global variables\n",
    "for key, df in dataframes.items():\n",
    "    globals()[key] = df\n",
    "\n",
    "# Display the keys of the dataframes dictionary to verify\n",
    "print(\"Loaded DataFrames:\", list(dataframes.keys()))\n",
    "\n",
    "# concat all motion data\n",
    "all_weeks = pd.concat([week1, week2, week3, week4, week5, week6, week7, week8], ignore_index=True)\n",
    "all_weeks = all_weeks.merge(players[['nflId', 'displayName', 'officialPosition']], on='nflId', how='left')\n",
    "\n",
    "# group all motion data by gameId, playId, nflId, displayName, officialPosition, then calculate mean, max, std of s, a, x, y\n",
    "motion_df = all_weeks.groupby(['gameId', 'playId', 'nflId', 'displayName', 'officialPosition']).agg({\n",
    "    's': ['mean', 'max', 'std'],\n",
    "    'a': ['mean', 'max', 'std'],\n",
    "    'x': ['min', 'max', 'mean'],\n",
    "    'y': ['min', 'max', 'mean']\n",
    "})\n",
    "\n",
    "# flatten the multi-level columns\n",
    "motion_df = motion_df.reset_index()\n",
    "motion_df.columns = ['_'.join(col).strip() if col[1] else col[0] for col in motion_df.columns.values]\n",
    "\n",
    "# Merge pffScoutingData into motion_df\n",
    "motion_df = motion_df.merge(pffScoutingData, on=['gameId', 'playId', 'nflId'], how='left')\n",
    "motion_df = motion_df.merge(plays, on=['gameId', 'playId'], how='left')\n",
    "ol = ['C', 'G', 'T', 'TE']\n",
    "dl = ['DL', 'LB', 'DE', 'OLB', 'ILB', 'NT', 'MLB']\n",
    "motion_df['OL'] = motion_df['officialPosition'].apply(lambda x: 1 if x in ol else 0)\n",
    "motion_df['DL'] = motion_df['officialPosition'].apply(lambda x: 1 if x in dl else 0)\n",
    "\n",
    "#create separate dataframes for OL and DL\n",
    "ol_motion_df = motion_df[motion_df['OL'] == 1].copy()\n",
    "dl_motion_df = motion_df[motion_df['DL'] == 1].copy()\n",
    "\n",
    "\n",
    "# set up dataset\n",
    "feature_cols = ['s_mean', 's_max', 's_std', 'a_mean', 'a_max', 'a_std', 'x_min',\n",
    "       'x_max', 'x_mean', 'y_min', 'y_max', 'y_mean', 'defendersInBox', 'absoluteYardlineNumber',\n",
    "       'pff_beatenByDefender', 'pff_hitAllowed', 'pff_hurryAllowed',\n",
    "       'pff_sackAllowed', 'pff_blockType', 'pff_backFieldBlock', 'playResult']\n",
    "\n",
    "motion_model = motion_df[feature_cols].copy()\n",
    "motion_model = motion_model.dropna(subset=['pff_blockType', 'pff_backFieldBlock', 'defendersInBox'])\n",
    "\n",
    "X = motion_model.drop(columns=['playResult'])\n",
    "y = motion_model['playResult']\n",
    "\n",
    "X = pd.get_dummies(X, columns=['pff_blockType'], drop_first=True, dtype='int')\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c296a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "r2_rf = r2_score(y_test, y_pred_rf)\n",
    "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
    "print(f\"Random Forest R^2 score: {r2_rf:.4f}\")\n",
    "print(f\"Random Forest Mean Squared Error: {mse_rf:.4f}\")\n",
    "# Get feature importances\n",
    "feature_importance_rf = rf.feature_importances_\n",
    "importance_df_rf = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'importance': feature_importance_rf\n",
    "}).sort_values(by='importance', ascending=False)\n",
    "print(importance_df_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea91a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import (\n",
    "    train_test_split, \n",
    "    cross_val_score, \n",
    "    GridSearchCV, \n",
    "    RandomizedSearchCV, \n",
    "    RepeatedKFold\n",
    ")\n",
    "\n",
    "# Parameter grid for RandomForestRegressor tuning.\n",
    "param_tests_rf = {\n",
    "    'n_estimators': [50, 100, 150, 200, 250],\n",
    "    'max_depth': [None, 5, 10, 15, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': [0.5, 0.7, 1.0],\n",
    "    'bootstrap': [True, False],\n",
    "    'warm_start': [False, True]\n",
    "}\n",
    "\n",
    "# Record the start time.\n",
    "start_time = time.time()\n",
    "\n",
    "# Instantiate the RandomForestRegressor with a fixed random state.\n",
    "rf_model = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Set up cross-validation: here we use 5-fold cross-validation repeated 3 times.\n",
    "cv = RepeatedKFold(n_splits=3, n_repeats=1, random_state=42)\n",
    "\n",
    "# Set up the RandomizedSearchCV.\n",
    "# n_iter controls the number of random combinations to try.\n",
    "random_search_rf = RandomizedSearchCV(\n",
    "    estimator=rf_model,\n",
    "    param_distributions=param_tests_rf,\n",
    "    n_iter=100,                          # Adjust n_iter for more/less thorough search.\n",
    "    scoring='neg_mean_squared_error',   # We use negative MSE; will convert later.\n",
    "    cv=cv,\n",
    "    n_jobs=12,\n",
    "    verbose=1,                          # Verbose prints progress.\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit the model using the reduced training data for RandomForest.\n",
    "random_search_rf.fit(X_train, y_train)\n",
    "\n",
    "# Extract the results into a DataFrame.\n",
    "results_rf_df = pd.DataFrame(random_search_rf.cv_results_)\n",
    "\n",
    "# Convert negative MSE back to positive MSE and then compute RMSE.\n",
    "results_rf_df['mean_test_MSE'] = -results_rf_df['mean_test_score']\n",
    "results_rf_df['mean_test_RMSE'] = np.sqrt(results_rf_df['mean_test_MSE'])\n",
    "\n",
    "# Sort the results by RMSE (lowest is best).\n",
    "sorted_results_rf = results_rf_df.sort_values(by='mean_test_RMSE')\n",
    "\n",
    "# Display the top 10 parameter combinations (by RMSE).\n",
    "print(\"\\nTop 10 Parameter Combinations by RMSE:\")\n",
    "print(sorted_results_rf[['param_n_estimators', \n",
    "                           'param_max_depth', \n",
    "                           'param_min_samples_split',\n",
    "                           'param_min_samples_leaf', \n",
    "                           'param_max_features', \n",
    "                           'param_bootstrap',\n",
    "                           'param_warm_start',\n",
    "                           'mean_test_RMSE']].head(10))\n",
    "\n",
    "# Get and print the best parameters and corresponding RMSE.\n",
    "best_params_rf = random_search_rf.best_params_\n",
    "best_rmse_rf = np.sqrt(-random_search_rf.best_score_)\n",
    "\n",
    "print(f\"\\nBest Parameters: {best_params_rf}\")\n",
    "print(f\"Best RMSE: {best_rmse_rf:.4f}\")\n",
    "\n",
    "# Display the execution time.\n",
    "execution_time_rf = time.time() - start_time\n",
    "print(f\"\\nExecution Time: {execution_time_rf:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27dcc313",
   "metadata": {},
   "source": [
    "## Dataset 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6c005362",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def load_and_normalize(path):\n",
    "    \"\"\"Load a CSV, strip and lowercase its column names.\"\"\"\n",
    "    df = pd.read_csv(path)\n",
    "    df.columns = df.columns.str.strip().str.lower()\n",
    "    return df\n",
    "\n",
    "# 1. LOAD & NORMALIZE\n",
    "# -------------------\n",
    "games            = load_and_normalize('datasets/NFL-Punt-Analytics-Competition/game_data.csv')\n",
    "play_info        = load_and_normalize('datasets/NFL-Punt-Analytics-Competition/play_information.csv')\n",
    "player_roles     = load_and_normalize('datasets/NFL-Punt-Analytics-Competition/play_player_role_data.csv')\n",
    "player_positions = load_and_normalize('datasets/NFL-Punt-Analytics-Competition/player_punt_data.csv')\n",
    "video_review     = load_and_normalize('datasets/NFL-Punt-Analytics-Competition/video_review.csv')\n",
    "\n",
    "# NGS chunks for 2016 & 2017\n",
    "ngs_paths = [\n",
    "    'datasets/NFL-Punt-Analytics-Competition/NGS-2016-pre.csv',\n",
    "    'datasets/NFL-Punt-Analytics-Competition/NGS-2016-post.csv',\n",
    "    'datasets/NFL-Punt-Analytics-Competition/NGS-2016-reg-wk1-6.csv',\n",
    "    'datasets/NFL-Punt-Analytics-Competition/NGS-2016-reg-wk7-12.csv',\n",
    "    'datasets/NFL-Punt-Analytics-Competition/NGS-2016-reg-wk13-17.csv',\n",
    "    'datasets/NFL-Punt-Analytics-Competition/NGS-2017-pre.csv',\n",
    "    'datasets/NFL-Punt-Analytics-Competition/NGS-2017-post.csv',\n",
    "    'datasets/NFL-Punt-Analytics-Competition/NGS-2017-reg-wk1-6.csv',\n",
    "    'datasets/NFL-Punt-Analytics-Competition/NGS-2017-reg-wk7-12.csv',\n",
    "    'datasets/NFL-Punt-Analytics-Competition/NGS-2017-reg-wk13-17.csv',\n",
    "]\n",
    "ngs = pd.concat([load_and_normalize(p) for p in ngs_paths], ignore_index=True)\n",
    "\n",
    "\n",
    "# 2. CLEAN & CAST\n",
    "# ----------------\n",
    "\n",
    "# Parse any ISO‐style dates in games & play_info\n",
    "for df in (games, play_info):\n",
    "    if 'game_date' in df.columns:\n",
    "        df['game_date'] = pd.to_datetime(df['game_date'])\n",
    "\n",
    "# Make sure keys are ints\n",
    "for df in (player_roles, player_positions, video_review):\n",
    "    for col in ('gamekey', 'playid', 'gsisid'):\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype(int)\n",
    "\n",
    "# Parse NGS timestamps if present\n",
    "if 'time' in ngs.columns:\n",
    "    ngs['time'] = pd.to_datetime(ngs['time'])\n",
    "\n",
    "\n",
    "# 3. MERGE BASE TABLE\n",
    "# -------------------\n",
    "\n",
    "# Start from each player’s role in each play\n",
    "df = player_roles.copy()\n",
    "\n",
    "# Merge play-level data\n",
    "df = df.merge(\n",
    "    play_info,\n",
    "    on=['gamekey','playid'],\n",
    "    how='left',\n",
    "    validate='many_to_one'\n",
    ")\n",
    "\n",
    "# Merge game-level data\n",
    "df = df.merge(\n",
    "    games.drop(columns=['game_date'], errors='ignore'),\n",
    "    on='gamekey',\n",
    "    how='left',\n",
    "    validate='many_to_one'\n",
    ")\n",
    "\n",
    "# Merge typical football position\n",
    "if {'gamekey','gsisid','position'}.issubset(player_positions.columns):\n",
    "    df = df.merge(\n",
    "        player_positions[['gamekey','gsisid','position']],\n",
    "        on=['gamekey','gsisid'],\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "\n",
    "# 4. ADD VIDEO REVIEW AS FEATURES\n",
    "# --------------------------------\n",
    "\n",
    "# Select the video_review columns we want\n",
    "video_feats = video_review[[\n",
    "    'gamekey','playid','gsisid',\n",
    "    'player_activity_derived',\n",
    "    'turnover_related',\n",
    "    'primary_impact_type',\n",
    "    'primary_partner_activity_derived',\n",
    "    'friendly_fire'\n",
    "]]\n",
    "\n",
    "# Merge them in\n",
    "df = df.merge(\n",
    "    video_feats,\n",
    "    on=['gamekey','playid','gsisid'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Fill NaNs for non-injured rows\n",
    "for c in [\n",
    "    'player_activity_derived',\n",
    "    'turnover_related',\n",
    "    'primary_impact_type',\n",
    "    'primary_partner_activity_derived',\n",
    "    'friendly_fire'\n",
    "]:\n",
    "    df[c] = df[c].fillna('NoInjury')\n",
    "\n",
    "\n",
    "# 5. BUILD TARGET—ANY INJURY\n",
    "# --------------------------\n",
    "\n",
    "inj = video_review[['gamekey','playid','gsisid']].copy()\n",
    "inj['injury'] = 1\n",
    "\n",
    "df = df.merge(inj, on=['gamekey','playid','gsisid'], how='left')\n",
    "df['injury'] = df['injury'].fillna(0).astype(int)\n",
    "\n",
    "\n",
    "# 6. AGGREGATE NGS INTO SUMMARY FEATURES\n",
    "# ---------------------------------------\n",
    "\n",
    "if {'gamekey','playid','gsisid','dis'}.issubset(ngs.columns):\n",
    "    ngs_summary = (\n",
    "        ngs\n",
    "        .groupby(['gamekey','playid','gsisid'], as_index=False)\n",
    "        .agg(\n",
    "            total_distance = ('dis','sum'),\n",
    "            max_step       = ('dis','max'),\n",
    "            mean_step      = ('dis','mean'),\n",
    "            n_timestamps   = ('time' if 'time' in ngs.columns else 'dis','count')\n",
    "        )\n",
    "    )\n",
    "    df = df.merge(ngs_summary, on=['gamekey','playid','gsisid'], how='left')\n",
    "else:\n",
    "    df[['total_distance','max_step','mean_step','n_timestamps']] = 0\n",
    "\n",
    "\n",
    "# 7. FINAL PREP FOR EDA & MODELING\n",
    "# ---------------------------------\n",
    "\n",
    "# Fill any remaining nulls in the summary stats\n",
    "for col in ['total_distance','max_step','mean_step','n_timestamps']:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].fillna(0)\n",
    "\n",
    "# Define feature columns\n",
    "feature_cols = [\n",
    "    'position', 'role', 'season_year', 'week',\n",
    "    'player_activity_derived',\n",
    "    'turnover_related',\n",
    "    'primary_impact_type',\n",
    "    'primary_partner_activity_derived',\n",
    "    'friendly_fire',\n",
    "    'total_distance','max_step','mean_step','n_timestamps'\n",
    "]\n",
    "feature_cols = [c for c in feature_cols if c in df.columns]\n",
    "\n",
    "# One‑hot encode categoricals\n",
    "X = pd.get_dummies(df[feature_cols], drop_first=True)\n",
    "y = df['injury']\n",
    "\n",
    "# Quick check\n",
    "print(\"▶️  Data prep complete!\")\n",
    "print(\"   X shape:\", X.shape)\n",
    "print(\"   Injury prevalence:\", y.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "60364a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {\n",
    "  'PR':'Returner',\n",
    "  'GL':'Gunner','GR':'Gunner',\n",
    "  'PLW':'Wing','PLG':'Wing','PRG':'Wing','PLL':'Wing',\n",
    "  # …etc…\n",
    "}\n",
    "df['role_group'] = df['role'].map(mapping)\n",
    "\n",
    "# 2) Drop all the old one‑hot role_ columns\n",
    "old_roles = [c for c in X.columns if c.startswith('role_')]\n",
    "X = X.drop(columns=old_roles)\n",
    "\n",
    "# 3) Add one‑hot encoding for the three groups\n",
    "role_group_dummies = pd.get_dummies(df['role_group'], prefix='role_group', drop_first=True)\n",
    "X = pd.concat([X.reset_index(drop=True), role_group_dummies.reset_index(drop=True)], axis=1)\n",
    "\n",
    "print(\"New feature set:\")\n",
    "print(X.filter(like='role_group_').columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "42752c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify and drop all columns that came from video_review\n",
    "leak_cols = [c for c in X.columns if \n",
    "             c.startswith('player_activity_derived_') or\n",
    "             c.startswith('turnover_related_') or\n",
    "             c.startswith('primary_impact_type_') or\n",
    "             c.startswith('primary_partner_activity_derived_') or\n",
    "             c.startswith('friendly_fire_')]\n",
    "\n",
    "X_nomap = X.drop(columns=leak_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9abb1c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scikit-learn (Machine Learning)\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split, \n",
    "    cross_val_score, \n",
    "    GridSearchCV, \n",
    "    RandomizedSearchCV, \n",
    "    RepeatedKFold\n",
    ")\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, \n",
    "    confusion_matrix, \n",
    "    classification_report\n",
    ")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_nomap, y, test_size=0.25, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_tr = scaler.transform(X_train)\n",
    "X_te = scaler.transform(X_test)\n",
    "\n",
    "# Initialize and train a RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_estimators=100, max_depth=10, class_weight='balanced', random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = rf.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(\"Test Accuracy:\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3a3b068f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Pull out the importances and map to feature names\n",
    "importances = rf.feature_importances_\n",
    "feat_imp = pd.Series(importances, index=X_train.columns)\n",
    "\n",
    "# 3. Sort and display the top 10\n",
    "top10 = feat_imp.sort_values(ascending=False).head(10)\n",
    "print(top10)\n",
    "\n",
    "# 4. (Optional) Plot as a horizontal bar chart\n",
    "plt.figure(figsize=(8, 5))\n",
    "top10.sort_values().plot.barh()\n",
    "plt.title(\"Top 10 Feature Importances\")\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8294f8e0",
   "metadata": {},
   "source": [
    "## Model Rerun with Class Balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c84f2511",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random forest with SMOTE\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# e.g. make injuries 50% of the training set\n",
    "smote = SMOTE(sampling_strategy=0.5, random_state=42)  \n",
    "\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Initialize and train a RandomForestClassifier\n",
    "rf_smote = RandomForestClassifier(n_estimators=100, max_depth=10, class_weight ='balanced_subsample', random_state=42)\n",
    "rf_smote.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = rf_smote.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(\"Test Accuracy:\", acc)\n",
    "\n",
    "\n",
    "# 8. Predict and evaluate\n",
    "y_pred = rf_smote.predict(X_test)\n",
    "y_pred_proba = rf_smote.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Classification report\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot()\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549d7b9f",
   "metadata": {},
   "source": [
    "## GridsearchCV with SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463e6eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "# Start timing\n",
    "start_time = time.time()\n",
    "\n",
    "# Define the pipeline: first SMOTE, then the classifier.\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('smote', SMOTE(random_state=42)),\n",
    "    ('classifier', RandomForestClassifier(random_state=0))\n",
    "])\n",
    "\n",
    "# Parameter grid for grid search.\n",
    "grid_params = {\n",
    "    'classifier__max_depth': [2, 3, 4, 5, None],\n",
    "    'classifier__min_samples_leaf': [1, 2, 3],\n",
    "    'classifier__min_samples_split': [2, 3, 4],\n",
    "    'classifier__max_features': [2, 3, 4],\n",
    "    'classifier__n_estimators': [75, 100, 125, 150]\n",
    "}\n",
    "\n",
    "# Define the scoring metrics (ensure 'scoring' is defined appropriately).\n",
    "scoring = {\n",
    "    'precision': 'precision',\n",
    "    'recall': 'recall',\n",
    "    'f1': 'f1'\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV.\n",
    "grid_cv = GridSearchCV(pipeline, \n",
    "                       grid_params, \n",
    "                       scoring=scoring, \n",
    "                       cv=3,\n",
    "                       n_jobs=-1, \n",
    "                       refit='f1',\n",
    "                       verbose=2)\n",
    "\n",
    "# Fit on the training data.\n",
    "grid_cv.fit(X_train, y_train)\n",
    "\n",
    "# Print execution time.\n",
    "execution_time = time.time() - start_time\n",
    "print(f\"\\nExecution Time: {execution_time:.2f}s\\n\")\n",
    "\n",
    "# Print the best parameters and best f1 score.\n",
    "print(\"Best Parameters:\")\n",
    "print(grid_cv.best_params_)\n",
    "print(f\"\\nBest F1 Score (CV): {grid_cv.best_score_:.4f}\\n\")\n",
    "\n",
    "# Display all cross-validation results in a sorted DataFrame.\n",
    "cv_results = pd.DataFrame(grid_cv.cv_results_)\n",
    "# Sort by rank_test_f1 (lowest rank is the best)\n",
    "cv_results = cv_results.sort_values('rank_test_f1')\n",
    "print(\"Grid Search CV Results (top 5 rows):\")\n",
    "print(cv_results[['params', 'mean_test_f1', 'std_test_f1', 'rank_test_f1']].head())\n",
    "\n",
    "# If you have a test set, evaluate on it:\n",
    "if 'X_test' in globals() and 'y_test' in globals():\n",
    "    y_pred = grid_cv.predict(X_test)\n",
    "    print(\"\\nClassification Report on Test Set:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    print(\"Confusion Matrix on Test Set:\")\n",
    "    print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64bd3e8-0fbb-4e75-bd4b-6fc8d195fe15",
   "metadata": {},
   "source": [
    "# 4. Storytelling With Data plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551a6ca9-6bba-4e0e-89a1-38008ad50823",
   "metadata": {},
   "source": [
    "Reproduce any graph of your choice in chapter seven (p. 165-185) of the Storytelling With Data book as best you can. You do not have to get the exact data values right, just the overall look and feel.\n",
    "\n",
    "PS Note: No graphs in the indicated Section, I picked one from chapter 8 instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "462c266a-e635-4bcb-971c-83961b9db9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "products = ['Product A', 'Product B', 'Product C', 'Product D', 'Product E']\n",
    "\n",
    "product_years = {\n",
    "    'Product A': [2008, 2010, 2014],\n",
    "    'Product B': [2008, 2010, 2014],\n",
    "    'Product C': [2010, 2012, 2014],\n",
    "    'Product D': [2011, 2012.5, 2014],\n",
    "    'Product E': [2013, 2013.5, 2014]\n",
    "}\n",
    "\n",
    "product_prices = {\n",
    "    'Product A': [420, 410, 280],\n",
    "    'Product B': [400, 390, 260],\n",
    "    'Product C': [100, 230, 200],\n",
    "    'Product D': [150, 250, 210],\n",
    "    'Product E': [90, 200, 220]\n",
    "}\n",
    "\n",
    "# Plot setup\n",
    "plt.figure(figsize=(10, 5))\n",
    "for idx, product in enumerate(products):\n",
    "    # Offset x values to group by product index\n",
    "    x_vals = np.linspace(idx, idx + 0.4, len(product_years[product]))\n",
    "    y_vals = product_prices[product]\n",
    "    years = product_years[product]\n",
    "\n",
    "    # Draw the line\n",
    "    plt.plot(x_vals, y_vals, color='gray', linewidth=5, alpha=0.8)\n",
    "\n",
    "    # Annotate beginning and end years\n",
    "    plt.text(x_vals[0], y_vals[0]+10, str(years[0]), color='gray', ha='center')\n",
    "    plt.text(x_vals[-1], y_vals[-1]-20, str(years[-1]), color='gray', ha='center')\n",
    "\n",
    "plt.xticks(range(len(products)), products)\n",
    "plt.yticks(range(0, 501, 100), [f\"${v}\" for v in range(0, 501, 100)])\n",
    "plt.ylim(0, 500)\n",
    "plt.title(\"Average Retail Product Price per Year\", fontsize=14, weight='bold')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.4)\n",
    "plt.box(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b4eb1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (uv)",
   "language": "python",
   "name": "uv"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
