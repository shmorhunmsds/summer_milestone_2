{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e53aadef",
   "metadata": {},
   "source": [
    "### 9.2 Activity: Jupyter Notebook â€“ Gradient Boost\n",
    "\n",
    "For this week, include ideas such as gradient boost, learning rate, number of estimators, tree depth, and regularization. This homework should be submitted for peer review in the assignment titled 9.3 Peer Review: Week 9 Jupyter Notebook. Complete and submit your Jupyter Notebook homework by 11:59pm ET on Sunday. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788118f8",
   "metadata": {},
   "source": [
    "Ok the plan:\n",
    "- Try a vanilla GBM on all 3 datasets, with some tuning\n",
    "- Go ahead and drop in XGBoost Classifier and XGBoost Regressor and try some model tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5c22f74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: NVIDIA GeForce RTX 5090 (UUID: GPU-f7f57dfb-5480-a1d6-1870-ac9f5f47ce36)\n"
     ]
    }
   ],
   "source": [
    "! nvidia-smi -L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "908215d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6cab9ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('scripts/play_surface/motion_train_X_resampled.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eab22345",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "128a29aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating lag/lead features...\n",
      "Creating statistical features...\n",
      "Creating interaction features...\n",
      "Defragmenting dataframe...\n",
      "Final shape: (9130, 477)\n",
      "Creating lag/lead features...\n",
      "Creating statistical features...\n",
      "Creating interaction features...\n",
      "Defragmenting dataframe...\n",
      "Final shape: (53392, 477)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_22130/3885097112.py:86: RuntimeWarning: invalid value encountered in divide\n",
      "  stds / np.abs(means),\n",
      "/tmp/ipykernel_22130/3885097112.py:86: RuntimeWarning: invalid value encountered in divide\n",
      "  stds / np.abs(means),\n",
      "/tmp/ipykernel_22130/3885097112.py:139: RuntimeWarning: divide by zero encountered in divide\n",
      "  new_features['total_displacement'] / total_distance,\n"
     ]
    }
   ],
   "source": [
    "# training sets\n",
    "X_train = pd.read_csv('scripts/play_surface/motion_train_X_resampled.csv')\n",
    "y_train = pd.read_csv('scripts/play_surface/motion_train_y_resampled.csv')['injured']\n",
    "\n",
    "# holdout sets\n",
    "X_holdout = pd.read_csv('scripts/play_surface/motion_holdout_X.csv')\n",
    "y_holdout = pd.read_csv('scripts/play_surface/motion_holdout_y.csv')['injured']\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def create_lag_features_optimized(df):\n",
    "    \"\"\"Optimized lag feature creation - builds all features at once\"\"\"\n",
    "    variables = ['dir', 'dis', 'o', 's', 'x', 'y']\n",
    "    \n",
    "    # Collect all new columns in a list first\n",
    "    new_columns = {}\n",
    "    \n",
    "    for var in variables:\n",
    "        cols = [f'{var}_slice_{i}' for i in range(10)]\n",
    "        \n",
    "        # Create lag features (previous time steps)\n",
    "        for lag in [1, 2, 3]:\n",
    "            for i in range(lag, 10):\n",
    "                col_name = f'{var}_slice_{i}_lag{lag}'\n",
    "                new_columns[col_name] = df[f'{var}_slice_{i-lag}'].values\n",
    "        \n",
    "        # Create lead features (future time steps)  \n",
    "        for lead in [1, 2]:\n",
    "            for i in range(10-lead):\n",
    "                col_name = f'{var}_slice_{i}_lead{lead}'\n",
    "                new_columns[col_name] = df[f'{var}_slice_{i+lead}'].values\n",
    "    \n",
    "    # Create new dataframe with all lag/lead features at once\n",
    "    new_features_df = pd.DataFrame(new_columns, index=df.index)\n",
    "    \n",
    "    # Concatenate with original dataframe\n",
    "    result_df = pd.concat([df, new_features_df], axis=1)\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "def create_time_series_features_optimized(df):\n",
    "    \"\"\"Optimized statistical time series feature creation\"\"\"\n",
    "    variables = ['dir', 'dis', 'o', 's', 'x', 'y']\n",
    "\n",
    "    # Collect all new features\n",
    "    new_features = {}\n",
    "    \n",
    "    for var in variables:\n",
    "        cols = [f'{var}_slice_{i}' for i in range(10)]\n",
    "        \n",
    "        # Get the data matrix for this variable (all time slices)\n",
    "        data_matrix = df[cols].values\n",
    "        \n",
    "        # Compute all statistical features vectorized\n",
    "        new_features[f'{var}_trend'] = np.nanmean(np.diff(data_matrix, axis=1), axis=1)\n",
    "        new_features[f'{var}_slope'] = (data_matrix[:, -1] - data_matrix[:, 0]) / 9\n",
    "        new_features[f'{var}_mean'] = np.nanmean(data_matrix, axis=1)\n",
    "        new_features[f'{var}_std'] = np.nanstd(data_matrix, axis=1)\n",
    "        new_features[f'{var}_min'] = np.nanmin(data_matrix, axis=1)\n",
    "        new_features[f'{var}_max'] = np.nanmax(data_matrix, axis=1)\n",
    "        new_features[f'{var}_range'] = new_features[f'{var}_max'] - new_features[f'{var}_min']\n",
    "        \n",
    "        # Change point features\n",
    "        new_features[f'{var}_early_mean'] = np.nanmean(data_matrix[:, :3], axis=1)\n",
    "        new_features[f'{var}_late_mean'] = np.nanmean(data_matrix[:, 7:], axis=1)\n",
    "        new_features[f'{var}_early_late_diff'] = new_features[f'{var}_late_mean'] - new_features[f'{var}_early_mean']\n",
    "        \n",
    "        # Volatility features\n",
    "        new_features[f'{var}_volatility'] = np.nanstd(np.diff(data_matrix, axis=1), axis=1)\n",
    "        \n",
    "        # Pattern recognition features\n",
    "        diff_matrix = np.diff(data_matrix, axis=1)\n",
    "        new_features[f'{var}_monotonic_increase'] = np.sum(diff_matrix > 0, axis=1)\n",
    "        new_features[f'{var}_monotonic_decrease'] = np.sum(diff_matrix < 0, axis=1)\n",
    "        \n",
    "        # Direction changes (second derivative)\n",
    "        second_diff = np.diff(diff_matrix, axis=1)\n",
    "        new_features[f'{var}_direction_changes'] = np.sum(second_diff != 0, axis=1)\n",
    "        \n",
    "        # Coefficient of variation (handle division by zero)\n",
    "        means = new_features[f'{var}_mean']\n",
    "        stds = new_features[f'{var}_std']\n",
    "        new_features[f'{var}_coefficient_variation'] = np.where(\n",
    "            np.abs(means) > 1e-8, \n",
    "            stds / np.abs(means), \n",
    "            0\n",
    "        )\n",
    "        \n",
    "        # Time-weighted features (recent values matter more)\n",
    "        weights = np.arange(1, 11)  # 1, 2, 3, ..., 10\n",
    "        weighted_sum = np.sum(data_matrix * weights, axis=1)\n",
    "        new_features[f'{var}_weighted_mean'] = weighted_sum / np.sum(weights)\n",
    "    \n",
    "    # Create dataframe from all new features\n",
    "    new_features_df = pd.DataFrame(new_features, index=df.index)\n",
    "    \n",
    "    # Concatenate with original\n",
    "    result_df = pd.concat([df, new_features_df], axis=1)\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "def create_interaction_features_optimized(df):\n",
    "    \"\"\"Optimized interaction feature creation\"\"\"\n",
    "    new_features = {}\n",
    "    \n",
    "    # Cross-variable interactions at each time slice\n",
    "    for i in range(10):\n",
    "        new_features[f'x_y_interaction_{i}'] = df[f'x_slice_{i}'] * df[f'y_slice_{i}']\n",
    "        new_features[f'dis_o_interaction_{i}'] = df[f'dis_slice_{i}'] * df[f'o_slice_{i}']\n",
    "        \n",
    "        # Distance-like features\n",
    "        new_features[f'euclidean_distance_{i}'] = np.sqrt(\n",
    "            df[f'x_slice_{i}']**2 + df[f'y_slice_{i}']**2\n",
    "        )\n",
    "        \n",
    "        # Angle-like features (if o represents orientation)\n",
    "        new_features[f'velocity_x_{i}'] = df[f'dis_slice_{i}'] * np.cos(df[f'o_slice_{i}'])\n",
    "        new_features[f'velocity_y_{i}'] = df[f'dis_slice_{i}'] * np.sin(df[f'o_slice_{i}'])\n",
    "    \n",
    "    # Multi-variable aggregate features\n",
    "    x_cols = [f'x_slice_{i}' for i in range(10)]\n",
    "    y_cols = [f'y_slice_{i}' for i in range(10)]\n",
    "    dis_cols = [f'dis_slice_{i}' for i in range(10)]\n",
    "    \n",
    "    # Total displacement path\n",
    "    x_data = df[x_cols].values\n",
    "    y_data = df[y_cols].values\n",
    "    \n",
    "    new_features['total_displacement'] = np.sqrt(\n",
    "        (x_data[:, -1] - x_data[:, 0])**2 + \n",
    "        (y_data[:, -1] - y_data[:, 0])**2\n",
    "    )\n",
    "    \n",
    "    # Path efficiency (straight line vs actual path)\n",
    "    total_distance = np.sum(df[dis_cols].values, axis=1)\n",
    "    new_features['path_efficiency'] = np.where(\n",
    "        total_distance > 1e-8,\n",
    "        new_features['total_displacement'] / total_distance,\n",
    "        0\n",
    "    )\n",
    "    \n",
    "    # Create dataframe and concatenate\n",
    "    new_features_df = pd.DataFrame(new_features, index=df.index)\n",
    "    result_df = pd.concat([df, new_features_df], axis=1)\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "def create_all_time_series_features(df):\n",
    "    \"\"\"Create all time series features efficiently\"\"\"\n",
    "    print(\"Creating lag/lead features...\")\n",
    "    df = create_lag_features_optimized(df)\n",
    "    \n",
    "    print(\"Creating statistical features...\")\n",
    "    df = create_time_series_features_optimized(df)\n",
    "    \n",
    "    print(\"Creating interaction features...\")\n",
    "    df = create_interaction_features_optimized(df)\n",
    "    \n",
    "    # Defragment the dataframe\n",
    "    print(\"Defragmenting dataframe...\")\n",
    "    df = df.copy()\n",
    "    \n",
    "    print(f\"Final shape: {df.shape}\")\n",
    "    return df\n",
    "\n",
    "# Usage:\n",
    "# df_enhanced = create_all_time_series_features(df)\n",
    "# \n",
    "# This will be much faster and won't generate performance warnings!\n",
    "\n",
    "X_train = create_all_time_series_features(X_train)\n",
    "X_holdout = create_all_time_series_features(X_holdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9c3d98e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score, train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix as conf_matrix, roc_auc_score, log_loss\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def diagnose_and_train_xgboost_classifier(X, y, use_time_split=False, test_size=0.2):\n",
    "    \"\"\"\n",
    "    Comprehensive XGBoost CLASSIFICATION training with diagnostics\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : DataFrame - Features\n",
    "    y : Series/array - Target variable (class labels)\n",
    "    use_time_split : bool - Whether to use TimeSeriesSplit (only if rows are temporally ordered)\n",
    "    test_size : float - Test set proportion\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=== DATA DIAGNOSTICS ===\")\n",
    "    print(f\"Features shape: {X.shape}\")\n",
    "    print(f\"Target shape: {y.shape}\")\n",
    "    print(f\"Target classes and counts:\")\n",
    "    print(pd.Series(y).value_counts().sort_index())\n",
    "    print(f\"Class balance:\")\n",
    "    for class_val in sorted(pd.Series(y).unique()):\n",
    "        prop = (pd.Series(y) == class_val).mean()\n",
    "        print(f\"  Class {class_val}: {prop:.3f} ({prop*100:.1f}%)\")\n",
    "    \n",
    "    # Determine if binary or multiclass\n",
    "    n_classes = len(pd.Series(y).unique())\n",
    "    is_binary = n_classes == 2\n",
    "    print(f\"Number of classes: {n_classes}\")\n",
    "    print(f\"Problem type: {'Binary' if is_binary else 'Multiclass'} Classification\")\n",
    "    \n",
    "    # Split data\n",
    "    if use_time_split:\n",
    "        print(\"\\n=== USING TIME SERIES SPLIT ===\")\n",
    "        # For actual time series data where row order matters\n",
    "        split_point = int(len(X) * (1 - test_size))\n",
    "        X_train, X_test = X.iloc[:split_point], X.iloc[split_point:]\n",
    "        y_train, y_test = y.iloc[:split_point], y.iloc[split_point:]\n",
    "        cv_method = TimeSeriesSplit(n_splits=5)\n",
    "    else:\n",
    "        print(\"\\n=== USING RANDOM SPLIT ===\")\n",
    "        # For independent sequences (more likely for your data)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=test_size, random_state=42\n",
    "        )\n",
    "        cv_method = 5  # Regular 5-fold CV\n",
    "    \n",
    "    print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "    print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "    \n",
    "    # XGBoost parameters for classification\n",
    "    if is_binary:\n",
    "        params = {\n",
    "            'objective': 'binary:logistic',\n",
    "            'eval_metric': 'logloss',\n",
    "            'max_depth': 4,\n",
    "            'learning_rate': 0.1,\n",
    "            'n_estimators': 500,\n",
    "            'subsample': 0.8,\n",
    "            'colsample_bytree': 0.8,\n",
    "            'reg_alpha': 0.1,\n",
    "            'reg_lambda': 0.1,\n",
    "            'random_state': 42,\n",
    "            'device': 'cuda',      # GPU acceleration\n",
    "            'tree_method': 'hist', # Required for GPU\n",
    "            'n_jobs': 1           # Use 1 job when using GPU\n",
    "        }\n",
    "        model = xgb.XGBClassifier(**params)\n",
    "    else:\n",
    "        params = {\n",
    "            'objective': 'multi:softprob',\n",
    "            'eval_metric': 'mlogloss',\n",
    "            'max_depth': 4,\n",
    "            'learning_rate': 0.1,\n",
    "            'n_estimators': 500,\n",
    "            'subsample': 0.8,\n",
    "            'colsample_bytree': 0.8,\n",
    "            'reg_alpha': 0.1,\n",
    "            'reg_lambda': 0.1,\n",
    "            'random_state': 42,\n",
    "            'device': 'cuda',      # GPU acceleration\n",
    "            'tree_method': 'hist', # Required for GPU\n",
    "            'n_jobs': 1           # Use 1 job when using GPU\n",
    "        }\n",
    "        model = xgb.XGBClassifier(**params)\n",
    "    \n",
    "    print(\"\\n=== MODEL TRAINING ===\")\n",
    "    \n",
    "    # Cross-validation with appropriate scoring\n",
    "    scoring_metric = 'roc_auc' if is_binary else 'accuracy'\n",
    "    \n",
    "    if use_time_split:\n",
    "        cv_scores = []\n",
    "        for train_idx, val_idx in cv_method.split(X_train):\n",
    "            X_cv_train, X_cv_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "            y_cv_train, y_cv_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "            \n",
    "            temp_model = xgb.XGBClassifier(**params)\n",
    "            temp_model.fit(X_cv_train, y_cv_train, verbose=False)\n",
    "            \n",
    "            if is_binary:\n",
    "                y_pred_proba = temp_model.predict_proba(X_cv_val)[:, 1]\n",
    "                score = roc_auc_score(y_cv_val, y_pred_proba)\n",
    "            else:\n",
    "                score = temp_model.score(X_cv_val, y_cv_val)\n",
    "            cv_scores.append(score)\n",
    "    else:\n",
    "        cv_scores = cross_val_score(model, X_train, y_train, cv=cv_method, scoring=scoring_metric)\n",
    "    \n",
    "    print(f\"Cross-validation {scoring_metric} scores: {cv_scores}\")\n",
    "    print(f\"Mean CV {scoring_metric}: {np.mean(cv_scores):.4f} (+/- {np.std(cv_scores) * 2:.4f})\")\n",
    "    \n",
    "    # Train final model\n",
    "    model.fit(X_train, y_train, verbose=False)\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    \n",
    "    if is_binary:\n",
    "        y_pred_proba_train = model.predict_proba(X_train)[:, 1]\n",
    "        y_pred_proba_test = model.predict_proba(X_test)[:, 1]\n",
    "    else:\n",
    "        y_pred_proba_train = model.predict_proba(X_train)\n",
    "        y_pred_proba_test = model.predict_proba(X_test)\n",
    "    \n",
    "    print(\"\\n=== EVALUATION METRICS ===\")\n",
    "    print(\"Training Set:\")\n",
    "    print(f\"  Accuracy: {accuracy_score(y_train, y_pred_train):.4f}\")\n",
    "    if is_binary:\n",
    "        print(f\"  AUC-ROC: {roc_auc_score(y_train, y_pred_proba_train):.4f}\")\n",
    "        print(f\"  Log Loss: {log_loss(y_train, y_pred_proba_train):.4f}\")\n",
    "    else:\n",
    "        print(f\"  Log Loss: {log_loss(y_train, y_pred_proba_train):.4f}\")\n",
    "    \n",
    "    print(\"\\nTest Set:\")\n",
    "    print(f\"  Accuracy: {accuracy_score(y_test, y_pred_test):.4f}\")\n",
    "    if is_binary:\n",
    "        print(f\"  AUC-ROC: {roc_auc_score(y_test, y_pred_proba_test):.4f}\")\n",
    "        print(f\"  Log Loss: {log_loss(y_test, y_pred_proba_test):.4f}\")\n",
    "    else:\n",
    "        print(f\"  Log Loss: {log_loss(y_test, y_pred_proba_test):.4f}\")\n",
    "    \n",
    "    print(\"\\n=== CLASSIFICATION REPORT ===\")\n",
    "    print(classification_report(y_test, y_pred_test))\n",
    "    \n",
    "    print(\"\\n=== CONFUSION MATRIX ===\")\n",
    "    cm = conf_matrix(y_test, y_pred_test)\n",
    "    print(cm)\n",
    "    \n",
    "    # Feature importance\n",
    "    print(\"\\n=== TOP 10 FEATURE IMPORTANCES ===\")\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': X.columns,\n",
    "        'importance': model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(feature_importance.head(10))\n",
    "    \n",
    "    # Baseline comparison\n",
    "    majority_class = pd.Series(y_train).mode()[0]\n",
    "    baseline_accuracy = (pd.Series(y_test) == majority_class).mean()\n",
    "    \n",
    "    print(f\"\\n=== BASELINE COMPARISON ===\")\n",
    "    print(f\"Baseline accuracy (majority class): {baseline_accuracy:.4f}\")\n",
    "    print(f\"Model improvement over baseline: {accuracy_score(y_test, y_pred_test) - baseline_accuracy:.4f}\")\n",
    "    \n",
    "    return model, {\n",
    "        'cv_scores': cv_scores,\n",
    "        'test_accuracy': accuracy_score(y_test, y_pred_test),\n",
    "        'test_auc': roc_auc_score(y_test, y_pred_proba_test) if is_binary else None,\n",
    "        'test_logloss': log_loss(y_test, y_pred_proba_test),\n",
    "        'feature_importance': feature_importance,\n",
    "        'classification_report': classification_report(y_test, y_pred_test, output_dict=True),\n",
    "        'confusion_matrix': conf_matrix(y_test, y_pred_test),\n",
    "        'y_pred_test': y_pred_test,\n",
    "        'y_pred_proba_test': y_pred_proba_test,\n",
    "        'y_test': y_test\n",
    "    }\n",
    "\n",
    "# Alternative: Simple baseline models for comparison\n",
    "def test_baseline_classifiers(X, y):\n",
    "    \"\"\"Test simple baseline classification models\"\"\"\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.dummy import DummyClassifier\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    models = {\n",
    "        'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "        'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "        'Majority Class': DummyClassifier(strategy='most_frequent'),\n",
    "        'Stratified': DummyClassifier(strategy='stratified', random_state=42)\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        \n",
    "        results[name] = accuracy\n",
    "        print(f\"{name}: Accuracy = {accuracy:.4f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Usage examples:\n",
    "# model, results = diagnose_and_train_xgboost_classifier(X, y, use_time_split=False)\n",
    "# baseline_results = test_baseline_classifiers(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a3df1f7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_22130/4162151118.py:1: FutureWarning: Series.ravel is deprecated. The underlying array is already 1D, so ravel is not necessary.  Use `to_numpy()` for conversion to a numpy array instead.\n",
      "  model, results = diagnose_and_train_xgboost_classifier(X_train, y_train.ravel(), use_time_split=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DATA DIAGNOSTICS ===\n",
      "Features shape: (9130, 477)\n",
      "Target shape: (9130,)\n",
      "Target classes and counts:\n",
      "0.0    8300\n",
      "1.0     830\n",
      "Name: count, dtype: int64\n",
      "Class balance:\n",
      "  Class 0.0: 0.909 (90.9%)\n",
      "  Class 1.0: 0.091 (9.1%)\n",
      "Number of classes: 2\n",
      "Problem type: Binary Classification\n",
      "\n",
      "=== USING RANDOM SPLIT ===\n",
      "Training set: 7304 samples\n",
      "Test set: 1826 samples\n",
      "\n",
      "=== MODEL TRAINING ===\n",
      "Cross-validation roc_auc scores: [0.99856921 0.99812596 0.9977806  0.99755979 0.99831143]\n",
      "Mean CV roc_auc: 0.9981 (+/- 0.0007)\n",
      "\n",
      "=== EVALUATION METRICS ===\n",
      "Training Set:\n",
      "  Accuracy: 1.0000\n",
      "  AUC-ROC: 1.0000\n",
      "  Log Loss: 0.0015\n",
      "\n",
      "Test Set:\n",
      "  Accuracy: 0.9929\n",
      "  AUC-ROC: 0.9991\n",
      "  Log Loss: 0.0212\n",
      "\n",
      "=== CLASSIFICATION REPORT ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      1.00      1.00      1659\n",
      "         1.0       1.00      0.92      0.96       167\n",
      "\n",
      "    accuracy                           0.99      1826\n",
      "   macro avg       1.00      0.96      0.98      1826\n",
      "weighted avg       0.99      0.99      0.99      1826\n",
      "\n",
      "\n",
      "=== CONFUSION MATRIX ===\n",
      "[[1659    0]\n",
      " [  13  154]]\n",
      "\n",
      "=== TOP 10 FEATURE IMPORTANCES ===\n",
      "                              feature  importance\n",
      "138                  dis_slice_7_lag2    0.030059\n",
      "81                      PlayType_Rush    0.025679\n",
      "332                           dir_std    0.024749\n",
      "77                      PlayType_Pass    0.018421\n",
      "65   RosterPosition_Offensive Lineman    0.017609\n",
      "395                            x_mean    0.017435\n",
      "208                    s_slice_3_lag1    0.013406\n",
      "312                   y_slice_0_lead1    0.013008\n",
      "64          RosterPosition_Linebacker    0.012893\n",
      "249                    x_slice_3_lag1    0.012689\n",
      "\n",
      "=== BASELINE COMPARISON ===\n",
      "Baseline accuracy (majority class): 0.9085\n",
      "Model improvement over baseline: 0.0843\n"
     ]
    }
   ],
   "source": [
    "model, results = diagnose_and_train_xgboost_classifier(X_train, y_train.ravel(), use_time_split=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6e495f29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.93      0.96     53371\n",
      "         1.0       0.00      0.10      0.00        21\n",
      "\n",
      "    accuracy                           0.93     53392\n",
      "   macro avg       0.50      0.51      0.48     53392\n",
      "weighted avg       1.00      0.93      0.96     53392\n",
      "\n",
      "ROC AUC Score:\n",
      "0.5699804869953452\n",
      "Log Loss:\n",
      "0.16693435180738594\n",
      "Confusion Matrix:\n",
      "[[49778  3593]\n",
      " [   19     2]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_holdout_scaled = scaler.fit_transform(X_holdout)\n",
    "\n",
    "y_holdout_pred = model.predict(X_holdout_scaled)\n",
    "y_holdout_pred_proba = model.predict_proba(X_holdout_scaled)[:, 1]\n",
    "\n",
    "from sklearn.metrics import classification_report, roc_auc_score, log_loss, confusion_matrix as conf_matrix\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_holdout, y_holdout_pred))\n",
    "\n",
    "print(\"ROC AUC Score:\")\n",
    "print(roc_auc_score(y_holdout, y_holdout_pred_proba))\n",
    "\n",
    "print(\"Log Loss:\")\n",
    "print(log_loss(y_holdout, y_holdout_pred_proba))\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix(y_holdout, y_holdout_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8c08d340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data feature means:\n",
      "dir_slice_0            -0.020542\n",
      "dir_slice_1            -0.004256\n",
      "dir_slice_2            -0.009954\n",
      "dir_slice_3            -0.000207\n",
      "dir_slice_4             0.016740\n",
      "                          ...   \n",
      "euclidean_distance_9    1.251630\n",
      "velocity_x_9           -0.015801\n",
      "velocity_y_9            0.006444\n",
      "total_displacement      1.073746\n",
      "path_efficiency         1.423568\n",
      "Length: 477, dtype: float64\n",
      "\n",
      "Holdout data feature means:\n",
      "3.500822934519567e-17\n",
      "Significant difference in dir_slice_0: p=0.000000\n",
      "Significant difference in dir_slice_1: p=0.000000\n",
      "Significant difference in dir_slice_2: p=0.000000\n",
      "Significant difference in dir_slice_3: p=0.000000\n",
      "Significant difference in dir_slice_4: p=0.000000\n",
      "Significant difference in dir_slice_5: p=0.000000\n",
      "Significant difference in dir_slice_6: p=0.000000\n",
      "Significant difference in dir_slice_7: p=0.000000\n",
      "Significant difference in dir_slice_8: p=0.000000\n",
      "Significant difference in dir_slice_9: p=0.000000\n",
      "Significant difference in dis_slice_0: p=0.000000\n",
      "Significant difference in dis_slice_1: p=0.000000\n",
      "Significant difference in dis_slice_2: p=0.000000\n",
      "Significant difference in dis_slice_3: p=0.000000\n",
      "Significant difference in dis_slice_4: p=0.000000\n",
      "Significant difference in dis_slice_5: p=0.000000\n",
      "Significant difference in dis_slice_6: p=0.000000\n",
      "Significant difference in dis_slice_7: p=0.000000\n",
      "Significant difference in dis_slice_8: p=0.000000\n",
      "Significant difference in dis_slice_9: p=0.000000\n",
      "Significant difference in o_slice_0: p=0.000000\n",
      "Significant difference in o_slice_1: p=0.000000\n",
      "Significant difference in o_slice_2: p=0.000000\n",
      "Significant difference in o_slice_3: p=0.000000\n",
      "Significant difference in o_slice_4: p=0.000000\n",
      "Significant difference in o_slice_5: p=0.000000\n",
      "Significant difference in o_slice_6: p=0.000000\n",
      "Significant difference in o_slice_7: p=0.000000\n",
      "Significant difference in o_slice_8: p=0.000000\n",
      "Significant difference in o_slice_9: p=0.000000\n",
      "Significant difference in s_slice_0: p=0.000000\n",
      "Significant difference in s_slice_1: p=0.000000\n",
      "Significant difference in s_slice_2: p=0.000000\n",
      "Significant difference in s_slice_3: p=0.000000\n",
      "Significant difference in s_slice_4: p=0.000000\n",
      "Significant difference in s_slice_5: p=0.000000\n",
      "Significant difference in s_slice_6: p=0.000000\n",
      "Significant difference in s_slice_7: p=0.000000\n",
      "Significant difference in s_slice_8: p=0.000000\n",
      "Significant difference in s_slice_9: p=0.000000\n",
      "Significant difference in x_slice_0: p=0.000000\n",
      "Significant difference in x_slice_1: p=0.000000\n",
      "Significant difference in x_slice_2: p=0.000000\n",
      "Significant difference in x_slice_3: p=0.000000\n",
      "Significant difference in x_slice_4: p=0.000000\n",
      "Significant difference in x_slice_5: p=0.000000\n",
      "Significant difference in x_slice_6: p=0.000000\n",
      "Significant difference in x_slice_7: p=0.000000\n",
      "Significant difference in x_slice_8: p=0.000000\n",
      "Significant difference in x_slice_9: p=0.000000\n",
      "Significant difference in y_slice_0: p=0.000000\n",
      "Significant difference in y_slice_1: p=0.000000\n",
      "Significant difference in y_slice_2: p=0.000000\n",
      "Significant difference in y_slice_3: p=0.000000\n",
      "Significant difference in y_slice_4: p=0.000000\n",
      "Significant difference in y_slice_5: p=0.000000\n",
      "Significant difference in y_slice_6: p=0.000000\n",
      "Significant difference in y_slice_7: p=0.000000\n",
      "Significant difference in y_slice_8: p=0.000000\n",
      "Significant difference in y_slice_9: p=0.000000\n",
      "Significant difference in PlayerDay: p=0.000000\n",
      "Significant difference in PlayerGame: p=0.000000\n",
      "Significant difference in RosterPosition_Defensive Lineman: p=0.000000\n",
      "Significant difference in RosterPosition_Kicker: p=0.000000\n",
      "Significant difference in RosterPosition_Linebacker: p=0.000000\n",
      "Significant difference in RosterPosition_Offensive Lineman: p=0.000000\n",
      "Significant difference in RosterPosition_Quarterback: p=0.000000\n",
      "Significant difference in RosterPosition_Running Back: p=0.000000\n",
      "Significant difference in RosterPosition_Safety: p=0.000000\n",
      "Significant difference in RosterPosition_Tight End: p=0.000000\n",
      "Significant difference in RosterPosition_Wide Receiver: p=0.000000\n",
      "Significant difference in FieldType_Synthetic: p=0.000000\n",
      "Significant difference in PlayType_Extra Point: p=0.000000\n",
      "Significant difference in PlayType_Field Goal: p=0.000000\n",
      "Significant difference in PlayType_Kickoff: p=0.000000\n",
      "Significant difference in PlayType_Kickoff Not Returned: p=0.000000\n",
      "Significant difference in PlayType_Kickoff Returned: p=0.000000\n",
      "Significant difference in PlayType_Pass: p=0.000000\n",
      "Significant difference in PlayType_Punt: p=0.000000\n",
      "Significant difference in PlayType_Punt Not Returned: p=0.000000\n",
      "Significant difference in PlayType_Punt Returned: p=0.000000\n",
      "Significant difference in PlayType_Rush: p=0.000000\n",
      "Significant difference in PlayType_Unknown: p=0.000000\n",
      "Significant difference in dir_slice_1_lag1: p=0.000000\n",
      "Significant difference in dir_slice_2_lag1: p=0.000000\n",
      "Significant difference in dir_slice_3_lag1: p=0.000000\n",
      "Significant difference in dir_slice_4_lag1: p=0.000000\n",
      "Significant difference in dir_slice_5_lag1: p=0.000000\n",
      "Significant difference in dir_slice_6_lag1: p=0.000000\n",
      "Significant difference in dir_slice_7_lag1: p=0.000000\n",
      "Significant difference in dir_slice_8_lag1: p=0.000000\n",
      "Significant difference in dir_slice_9_lag1: p=0.000000\n",
      "Significant difference in dir_slice_2_lag2: p=0.000000\n",
      "Significant difference in dir_slice_3_lag2: p=0.000000\n",
      "Significant difference in dir_slice_4_lag2: p=0.000000\n",
      "Significant difference in dir_slice_5_lag2: p=0.000000\n",
      "Significant difference in dir_slice_6_lag2: p=0.000000\n",
      "Significant difference in dir_slice_7_lag2: p=0.000000\n",
      "Significant difference in dir_slice_8_lag2: p=0.000000\n",
      "Significant difference in dir_slice_9_lag2: p=0.000000\n",
      "Significant difference in dir_slice_3_lag3: p=0.000000\n",
      "Significant difference in dir_slice_4_lag3: p=0.000000\n",
      "Significant difference in dir_slice_5_lag3: p=0.000000\n",
      "Significant difference in dir_slice_6_lag3: p=0.000000\n",
      "Significant difference in dir_slice_7_lag3: p=0.000000\n",
      "Significant difference in dir_slice_8_lag3: p=0.000000\n",
      "Significant difference in dir_slice_9_lag3: p=0.000000\n",
      "Significant difference in dir_slice_0_lead1: p=0.000000\n",
      "Significant difference in dir_slice_1_lead1: p=0.000000\n",
      "Significant difference in dir_slice_2_lead1: p=0.000000\n",
      "Significant difference in dir_slice_3_lead1: p=0.000000\n",
      "Significant difference in dir_slice_4_lead1: p=0.000000\n",
      "Significant difference in dir_slice_5_lead1: p=0.000000\n",
      "Significant difference in dir_slice_6_lead1: p=0.000000\n",
      "Significant difference in dir_slice_7_lead1: p=0.000000\n",
      "Significant difference in dir_slice_8_lead1: p=0.000000\n",
      "Significant difference in dir_slice_0_lead2: p=0.000000\n",
      "Significant difference in dir_slice_1_lead2: p=0.000000\n",
      "Significant difference in dir_slice_2_lead2: p=0.000000\n",
      "Significant difference in dir_slice_3_lead2: p=0.000000\n",
      "Significant difference in dir_slice_4_lead2: p=0.000000\n",
      "Significant difference in dir_slice_5_lead2: p=0.000000\n",
      "Significant difference in dir_slice_6_lead2: p=0.000000\n",
      "Significant difference in dir_slice_7_lead2: p=0.000000\n",
      "Significant difference in dis_slice_1_lag1: p=0.000000\n",
      "Significant difference in dis_slice_2_lag1: p=0.000000\n",
      "Significant difference in dis_slice_3_lag1: p=0.000000\n",
      "Significant difference in dis_slice_4_lag1: p=0.000000\n",
      "Significant difference in dis_slice_5_lag1: p=0.000000\n",
      "Significant difference in dis_slice_6_lag1: p=0.000000\n",
      "Significant difference in dis_slice_7_lag1: p=0.000000\n",
      "Significant difference in dis_slice_8_lag1: p=0.000000\n",
      "Significant difference in dis_slice_9_lag1: p=0.000000\n",
      "Significant difference in dis_slice_2_lag2: p=0.000000\n",
      "Significant difference in dis_slice_3_lag2: p=0.000000\n",
      "Significant difference in dis_slice_4_lag2: p=0.000000\n",
      "Significant difference in dis_slice_5_lag2: p=0.000000\n",
      "Significant difference in dis_slice_6_lag2: p=0.000000\n",
      "Significant difference in dis_slice_7_lag2: p=0.000000\n",
      "Significant difference in dis_slice_8_lag2: p=0.000000\n",
      "Significant difference in dis_slice_9_lag2: p=0.000000\n",
      "Significant difference in dis_slice_3_lag3: p=0.000000\n",
      "Significant difference in dis_slice_4_lag3: p=0.000000\n",
      "Significant difference in dis_slice_5_lag3: p=0.000000\n",
      "Significant difference in dis_slice_6_lag3: p=0.000000\n",
      "Significant difference in dis_slice_7_lag3: p=0.000000\n",
      "Significant difference in dis_slice_8_lag3: p=0.000000\n",
      "Significant difference in dis_slice_9_lag3: p=0.000000\n",
      "Significant difference in dis_slice_0_lead1: p=0.000000\n",
      "Significant difference in dis_slice_1_lead1: p=0.000000\n",
      "Significant difference in dis_slice_2_lead1: p=0.000000\n",
      "Significant difference in dis_slice_3_lead1: p=0.000000\n",
      "Significant difference in dis_slice_4_lead1: p=0.000000\n",
      "Significant difference in dis_slice_5_lead1: p=0.000000\n",
      "Significant difference in dis_slice_6_lead1: p=0.000000\n",
      "Significant difference in dis_slice_7_lead1: p=0.000000\n",
      "Significant difference in dis_slice_8_lead1: p=0.000000\n",
      "Significant difference in dis_slice_0_lead2: p=0.000000\n",
      "Significant difference in dis_slice_1_lead2: p=0.000000\n",
      "Significant difference in dis_slice_2_lead2: p=0.000000\n",
      "Significant difference in dis_slice_3_lead2: p=0.000000\n",
      "Significant difference in dis_slice_4_lead2: p=0.000000\n",
      "Significant difference in dis_slice_5_lead2: p=0.000000\n",
      "Significant difference in dis_slice_6_lead2: p=0.000000\n",
      "Significant difference in dis_slice_7_lead2: p=0.000000\n",
      "Significant difference in o_slice_1_lag1: p=0.000000\n",
      "Significant difference in o_slice_2_lag1: p=0.000000\n",
      "Significant difference in o_slice_3_lag1: p=0.000000\n",
      "Significant difference in o_slice_4_lag1: p=0.000000\n",
      "Significant difference in o_slice_5_lag1: p=0.000000\n",
      "Significant difference in o_slice_6_lag1: p=0.000000\n",
      "Significant difference in o_slice_7_lag1: p=0.000000\n",
      "Significant difference in o_slice_8_lag1: p=0.000000\n",
      "Significant difference in o_slice_9_lag1: p=0.000000\n",
      "Significant difference in o_slice_2_lag2: p=0.000000\n",
      "Significant difference in o_slice_3_lag2: p=0.000000\n",
      "Significant difference in o_slice_4_lag2: p=0.000000\n",
      "Significant difference in o_slice_5_lag2: p=0.000000\n",
      "Significant difference in o_slice_6_lag2: p=0.000000\n",
      "Significant difference in o_slice_7_lag2: p=0.000000\n",
      "Significant difference in o_slice_8_lag2: p=0.000000\n",
      "Significant difference in o_slice_9_lag2: p=0.000000\n",
      "Significant difference in o_slice_3_lag3: p=0.000000\n",
      "Significant difference in o_slice_4_lag3: p=0.000000\n",
      "Significant difference in o_slice_5_lag3: p=0.000000\n",
      "Significant difference in o_slice_6_lag3: p=0.000000\n",
      "Significant difference in o_slice_7_lag3: p=0.000000\n",
      "Significant difference in o_slice_8_lag3: p=0.000000\n",
      "Significant difference in o_slice_9_lag3: p=0.000000\n",
      "Significant difference in o_slice_0_lead1: p=0.000000\n",
      "Significant difference in o_slice_1_lead1: p=0.000000\n",
      "Significant difference in o_slice_2_lead1: p=0.000000\n",
      "Significant difference in o_slice_3_lead1: p=0.000000\n",
      "Significant difference in o_slice_4_lead1: p=0.000000\n",
      "Significant difference in o_slice_5_lead1: p=0.000000\n",
      "Significant difference in o_slice_6_lead1: p=0.000000\n",
      "Significant difference in o_slice_7_lead1: p=0.000000\n",
      "Significant difference in o_slice_8_lead1: p=0.000000\n",
      "Significant difference in o_slice_0_lead2: p=0.000000\n",
      "Significant difference in o_slice_1_lead2: p=0.000000\n",
      "Significant difference in o_slice_2_lead2: p=0.000000\n",
      "Significant difference in o_slice_3_lead2: p=0.000000\n",
      "Significant difference in o_slice_4_lead2: p=0.000000\n",
      "Significant difference in o_slice_5_lead2: p=0.000000\n",
      "Significant difference in o_slice_6_lead2: p=0.000000\n",
      "Significant difference in o_slice_7_lead2: p=0.000000\n",
      "Significant difference in s_slice_1_lag1: p=0.000000\n",
      "Significant difference in s_slice_2_lag1: p=0.000000\n",
      "Significant difference in s_slice_3_lag1: p=0.000000\n",
      "Significant difference in s_slice_4_lag1: p=0.000000\n",
      "Significant difference in s_slice_5_lag1: p=0.000000\n",
      "Significant difference in s_slice_6_lag1: p=0.000000\n",
      "Significant difference in s_slice_7_lag1: p=0.000000\n",
      "Significant difference in s_slice_8_lag1: p=0.000000\n",
      "Significant difference in s_slice_9_lag1: p=0.000000\n",
      "Significant difference in s_slice_2_lag2: p=0.000000\n",
      "Significant difference in s_slice_3_lag2: p=0.000000\n",
      "Significant difference in s_slice_4_lag2: p=0.000000\n",
      "Significant difference in s_slice_5_lag2: p=0.000000\n",
      "Significant difference in s_slice_6_lag2: p=0.000000\n",
      "Significant difference in s_slice_7_lag2: p=0.000000\n",
      "Significant difference in s_slice_8_lag2: p=0.000000\n",
      "Significant difference in s_slice_9_lag2: p=0.000000\n",
      "Significant difference in s_slice_3_lag3: p=0.000000\n",
      "Significant difference in s_slice_4_lag3: p=0.000000\n",
      "Significant difference in s_slice_5_lag3: p=0.000000\n",
      "Significant difference in s_slice_6_lag3: p=0.000000\n",
      "Significant difference in s_slice_7_lag3: p=0.000000\n",
      "Significant difference in s_slice_8_lag3: p=0.000000\n",
      "Significant difference in s_slice_9_lag3: p=0.000000\n",
      "Significant difference in s_slice_0_lead1: p=0.000000\n",
      "Significant difference in s_slice_1_lead1: p=0.000000\n",
      "Significant difference in s_slice_2_lead1: p=0.000000\n",
      "Significant difference in s_slice_3_lead1: p=0.000000\n",
      "Significant difference in s_slice_4_lead1: p=0.000000\n",
      "Significant difference in s_slice_5_lead1: p=0.000000\n",
      "Significant difference in s_slice_6_lead1: p=0.000000\n",
      "Significant difference in s_slice_7_lead1: p=0.000000\n",
      "Significant difference in s_slice_8_lead1: p=0.000000\n",
      "Significant difference in s_slice_0_lead2: p=0.000000\n",
      "Significant difference in s_slice_1_lead2: p=0.000000\n",
      "Significant difference in s_slice_2_lead2: p=0.000000\n",
      "Significant difference in s_slice_3_lead2: p=0.000000\n",
      "Significant difference in s_slice_4_lead2: p=0.000000\n",
      "Significant difference in s_slice_5_lead2: p=0.000000\n",
      "Significant difference in s_slice_6_lead2: p=0.000000\n",
      "Significant difference in s_slice_7_lead2: p=0.000000\n",
      "Significant difference in x_slice_1_lag1: p=0.000000\n",
      "Significant difference in x_slice_2_lag1: p=0.000000\n",
      "Significant difference in x_slice_3_lag1: p=0.000000\n",
      "Significant difference in x_slice_4_lag1: p=0.000000\n",
      "Significant difference in x_slice_5_lag1: p=0.000000\n",
      "Significant difference in x_slice_6_lag1: p=0.000000\n",
      "Significant difference in x_slice_7_lag1: p=0.000000\n",
      "Significant difference in x_slice_8_lag1: p=0.000000\n",
      "Significant difference in x_slice_9_lag1: p=0.000000\n",
      "Significant difference in x_slice_2_lag2: p=0.000000\n",
      "Significant difference in x_slice_3_lag2: p=0.000000\n",
      "Significant difference in x_slice_4_lag2: p=0.000000\n",
      "Significant difference in x_slice_5_lag2: p=0.000000\n",
      "Significant difference in x_slice_6_lag2: p=0.000000\n",
      "Significant difference in x_slice_7_lag2: p=0.000000\n",
      "Significant difference in x_slice_8_lag2: p=0.000000\n",
      "Significant difference in x_slice_9_lag2: p=0.000000\n",
      "Significant difference in x_slice_3_lag3: p=0.000000\n",
      "Significant difference in x_slice_4_lag3: p=0.000000\n",
      "Significant difference in x_slice_5_lag3: p=0.000000\n",
      "Significant difference in x_slice_6_lag3: p=0.000000\n",
      "Significant difference in x_slice_7_lag3: p=0.000000\n",
      "Significant difference in x_slice_8_lag3: p=0.000000\n",
      "Significant difference in x_slice_9_lag3: p=0.000000\n",
      "Significant difference in x_slice_0_lead1: p=0.000000\n",
      "Significant difference in x_slice_1_lead1: p=0.000000\n",
      "Significant difference in x_slice_2_lead1: p=0.000000\n",
      "Significant difference in x_slice_3_lead1: p=0.000000\n",
      "Significant difference in x_slice_4_lead1: p=0.000000\n",
      "Significant difference in x_slice_5_lead1: p=0.000000\n",
      "Significant difference in x_slice_6_lead1: p=0.000000\n",
      "Significant difference in x_slice_7_lead1: p=0.000000\n",
      "Significant difference in x_slice_8_lead1: p=0.000000\n",
      "Significant difference in x_slice_0_lead2: p=0.000000\n",
      "Significant difference in x_slice_1_lead2: p=0.000000\n",
      "Significant difference in x_slice_2_lead2: p=0.000000\n",
      "Significant difference in x_slice_3_lead2: p=0.000000\n",
      "Significant difference in x_slice_4_lead2: p=0.000000\n",
      "Significant difference in x_slice_5_lead2: p=0.000000\n",
      "Significant difference in x_slice_6_lead2: p=0.000000\n",
      "Significant difference in x_slice_7_lead2: p=0.000000\n",
      "Significant difference in y_slice_1_lag1: p=0.000000\n",
      "Significant difference in y_slice_2_lag1: p=0.000000\n",
      "Significant difference in y_slice_3_lag1: p=0.000000\n",
      "Significant difference in y_slice_4_lag1: p=0.000000\n",
      "Significant difference in y_slice_5_lag1: p=0.000000\n",
      "Significant difference in y_slice_6_lag1: p=0.000000\n",
      "Significant difference in y_slice_7_lag1: p=0.000000\n",
      "Significant difference in y_slice_8_lag1: p=0.000000\n",
      "Significant difference in y_slice_9_lag1: p=0.000000\n",
      "Significant difference in y_slice_2_lag2: p=0.000000\n",
      "Significant difference in y_slice_3_lag2: p=0.000000\n",
      "Significant difference in y_slice_4_lag2: p=0.000000\n",
      "Significant difference in y_slice_5_lag2: p=0.000000\n",
      "Significant difference in y_slice_6_lag2: p=0.000000\n",
      "Significant difference in y_slice_7_lag2: p=0.000000\n",
      "Significant difference in y_slice_8_lag2: p=0.000000\n",
      "Significant difference in y_slice_9_lag2: p=0.000000\n",
      "Significant difference in y_slice_3_lag3: p=0.000000\n",
      "Significant difference in y_slice_4_lag3: p=0.000000\n",
      "Significant difference in y_slice_5_lag3: p=0.000000\n",
      "Significant difference in y_slice_6_lag3: p=0.000000\n",
      "Significant difference in y_slice_7_lag3: p=0.000000\n",
      "Significant difference in y_slice_8_lag3: p=0.000000\n",
      "Significant difference in y_slice_9_lag3: p=0.000000\n",
      "Significant difference in y_slice_0_lead1: p=0.000000\n",
      "Significant difference in y_slice_1_lead1: p=0.000000\n",
      "Significant difference in y_slice_2_lead1: p=0.000000\n",
      "Significant difference in y_slice_3_lead1: p=0.000000\n",
      "Significant difference in y_slice_4_lead1: p=0.000000\n",
      "Significant difference in y_slice_5_lead1: p=0.000000\n",
      "Significant difference in y_slice_6_lead1: p=0.000000\n",
      "Significant difference in y_slice_7_lead1: p=0.000000\n",
      "Significant difference in y_slice_8_lead1: p=0.000000\n",
      "Significant difference in y_slice_0_lead2: p=0.000000\n",
      "Significant difference in y_slice_1_lead2: p=0.000000\n",
      "Significant difference in y_slice_2_lead2: p=0.000000\n",
      "Significant difference in y_slice_3_lead2: p=0.000000\n",
      "Significant difference in y_slice_4_lead2: p=0.000000\n",
      "Significant difference in y_slice_5_lead2: p=0.000000\n",
      "Significant difference in y_slice_6_lead2: p=0.000000\n",
      "Significant difference in y_slice_7_lead2: p=0.000000\n",
      "Significant difference in dir_trend: p=0.000000\n",
      "Significant difference in dir_slope: p=0.000000\n",
      "Significant difference in dir_mean: p=0.000000\n",
      "Significant difference in dir_std: p=0.000000\n",
      "Significant difference in dir_min: p=0.000000\n",
      "Significant difference in dir_max: p=0.000000\n",
      "Significant difference in dir_range: p=0.000000\n",
      "Significant difference in dir_early_mean: p=0.000000\n",
      "Significant difference in dir_late_mean: p=0.000000\n",
      "Significant difference in dir_early_late_diff: p=0.000000\n",
      "Significant difference in dir_volatility: p=0.000000\n",
      "Significant difference in dir_coefficient_variation: p=0.000000\n",
      "Significant difference in dir_weighted_mean: p=0.000000\n",
      "Significant difference in dis_trend: p=0.000000\n",
      "Significant difference in dis_slope: p=0.000000\n",
      "Significant difference in dis_mean: p=0.000000\n",
      "Significant difference in dis_std: p=0.000000\n",
      "Significant difference in dis_min: p=0.000000\n",
      "Significant difference in dis_max: p=0.000000\n",
      "Significant difference in dis_range: p=0.000000\n",
      "Significant difference in dis_early_mean: p=0.000000\n",
      "Significant difference in dis_late_mean: p=0.000000\n",
      "Significant difference in dis_early_late_diff: p=0.000000\n",
      "Significant difference in dis_volatility: p=0.000000\n",
      "Significant difference in dis_monotonic_decrease: p=0.000000\n",
      "Significant difference in dis_direction_changes: p=0.000000\n",
      "Significant difference in dis_coefficient_variation: p=0.000000\n",
      "Significant difference in dis_weighted_mean: p=0.000000\n",
      "Significant difference in o_trend: p=0.000000\n",
      "Significant difference in o_slope: p=0.000000\n",
      "Significant difference in o_mean: p=0.000000\n",
      "Significant difference in o_std: p=0.000000\n",
      "Significant difference in o_min: p=0.000000\n",
      "Significant difference in o_max: p=0.000000\n",
      "Significant difference in o_range: p=0.000000\n",
      "Significant difference in o_early_mean: p=0.000000\n",
      "Significant difference in o_late_mean: p=0.000000\n",
      "Significant difference in o_early_late_diff: p=0.000000\n",
      "Significant difference in o_volatility: p=0.000000\n",
      "Significant difference in o_coefficient_variation: p=0.000000\n",
      "Significant difference in o_weighted_mean: p=0.000000\n",
      "Significant difference in s_trend: p=0.000000\n",
      "Significant difference in s_slope: p=0.000000\n",
      "Significant difference in s_mean: p=0.000000\n",
      "Significant difference in s_std: p=0.000000\n",
      "Significant difference in s_min: p=0.000000\n",
      "Significant difference in s_max: p=0.000000\n",
      "Significant difference in s_range: p=0.000000\n",
      "Significant difference in s_early_mean: p=0.000000\n",
      "Significant difference in s_late_mean: p=0.000000\n",
      "Significant difference in s_early_late_diff: p=0.000000\n",
      "Significant difference in s_volatility: p=0.000000\n",
      "Significant difference in s_monotonic_increase: p=0.000000\n",
      "Significant difference in s_monotonic_decrease: p=0.000000\n",
      "Significant difference in s_coefficient_variation: p=0.000000\n",
      "Significant difference in s_weighted_mean: p=0.000000\n",
      "Significant difference in x_trend: p=0.000000\n",
      "Significant difference in x_slope: p=0.000000\n",
      "Significant difference in x_mean: p=0.000000\n",
      "Significant difference in x_std: p=0.000000\n",
      "Significant difference in x_min: p=0.000000\n",
      "Significant difference in x_max: p=0.000000\n",
      "Significant difference in x_range: p=0.000000\n",
      "Significant difference in x_early_mean: p=0.000000\n",
      "Significant difference in x_late_mean: p=0.000000\n",
      "Significant difference in x_early_late_diff: p=0.000000\n",
      "Significant difference in x_volatility: p=0.000000\n",
      "Significant difference in x_monotonic_increase: p=0.000093\n",
      "Significant difference in x_monotonic_decrease: p=0.000021\n",
      "Significant difference in x_coefficient_variation: p=0.000000\n",
      "Significant difference in x_weighted_mean: p=0.000000\n",
      "Significant difference in y_trend: p=0.000000\n",
      "Significant difference in y_slope: p=0.000000\n",
      "Significant difference in y_mean: p=0.000000\n",
      "Significant difference in y_std: p=0.000000\n",
      "Significant difference in y_min: p=0.000000\n",
      "Significant difference in y_max: p=0.000000\n",
      "Significant difference in y_range: p=0.000000\n",
      "Significant difference in y_early_mean: p=0.000000\n",
      "Significant difference in y_late_mean: p=0.000000\n",
      "Significant difference in y_early_late_diff: p=0.000000\n",
      "Significant difference in y_volatility: p=0.000000\n",
      "Significant difference in y_monotonic_increase: p=0.000000\n",
      "Significant difference in y_monotonic_decrease: p=0.000000\n",
      "Significant difference in y_coefficient_variation: p=0.000000\n",
      "Significant difference in y_weighted_mean: p=0.000000\n",
      "Significant difference in x_y_interaction_0: p=0.000000\n",
      "Significant difference in dis_o_interaction_0: p=0.000000\n",
      "Significant difference in euclidean_distance_0: p=0.000000\n",
      "Significant difference in velocity_x_0: p=0.000000\n",
      "Significant difference in velocity_y_0: p=0.000000\n",
      "Significant difference in x_y_interaction_1: p=0.000000\n",
      "Significant difference in dis_o_interaction_1: p=0.000000\n",
      "Significant difference in euclidean_distance_1: p=0.000000\n",
      "Significant difference in velocity_x_1: p=0.000000\n",
      "Significant difference in velocity_y_1: p=0.000000\n",
      "Significant difference in x_y_interaction_2: p=0.000000\n",
      "Significant difference in dis_o_interaction_2: p=0.000000\n",
      "Significant difference in euclidean_distance_2: p=0.000000\n",
      "Significant difference in velocity_x_2: p=0.000000\n",
      "Significant difference in velocity_y_2: p=0.000000\n",
      "Significant difference in x_y_interaction_3: p=0.000000\n",
      "Significant difference in dis_o_interaction_3: p=0.000000\n",
      "Significant difference in euclidean_distance_3: p=0.000000\n",
      "Significant difference in velocity_x_3: p=0.000000\n",
      "Significant difference in velocity_y_3: p=0.000000\n",
      "Significant difference in x_y_interaction_4: p=0.000000\n",
      "Significant difference in dis_o_interaction_4: p=0.000000\n",
      "Significant difference in euclidean_distance_4: p=0.000000\n",
      "Significant difference in velocity_x_4: p=0.000000\n",
      "Significant difference in velocity_y_4: p=0.000000\n",
      "Significant difference in x_y_interaction_5: p=0.000000\n",
      "Significant difference in dis_o_interaction_5: p=0.000000\n",
      "Significant difference in euclidean_distance_5: p=0.000000\n",
      "Significant difference in velocity_x_5: p=0.000000\n",
      "Significant difference in velocity_y_5: p=0.000000\n",
      "Significant difference in x_y_interaction_6: p=0.000000\n",
      "Significant difference in dis_o_interaction_6: p=0.000000\n",
      "Significant difference in euclidean_distance_6: p=0.000000\n",
      "Significant difference in velocity_x_6: p=0.000000\n",
      "Significant difference in velocity_y_6: p=0.000000\n",
      "Significant difference in x_y_interaction_7: p=0.000000\n",
      "Significant difference in dis_o_interaction_7: p=0.000000\n",
      "Significant difference in euclidean_distance_7: p=0.000000\n",
      "Significant difference in velocity_x_7: p=0.000000\n",
      "Significant difference in velocity_y_7: p=0.000000\n",
      "Significant difference in x_y_interaction_8: p=0.000000\n",
      "Significant difference in dis_o_interaction_8: p=0.000000\n",
      "Significant difference in euclidean_distance_8: p=0.000000\n",
      "Significant difference in velocity_x_8: p=0.000000\n",
      "Significant difference in velocity_y_8: p=0.000000\n",
      "Significant difference in x_y_interaction_9: p=0.000000\n",
      "Significant difference in dis_o_interaction_9: p=0.000000\n",
      "Significant difference in euclidean_distance_9: p=0.000000\n",
      "Significant difference in velocity_x_9: p=0.000000\n",
      "Significant difference in velocity_y_9: p=0.000000\n",
      "Significant difference in total_displacement: p=0.000000\n",
      "Significant difference in path_efficiency: p=0.000000\n"
     ]
    }
   ],
   "source": [
    "# Compare feature distributions\n",
    "print(\"Training data feature means:\")\n",
    "print(X_train.mean())\n",
    "print(\"\\nHoldout data feature means:\")\n",
    "print(X_holdout_scaled.mean())\n",
    "\n",
    "# Check for significant differences\n",
    "from scipy.stats import ks_2samp\n",
    "for col in X_train.columns:\n",
    "    stat, p_value = ks_2samp(X_train[col], X_holdout[col])\n",
    "    if p_value < 0.01:\n",
    "        print(f\"Significant difference in {col}: p={p_value:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37cced61",
   "metadata": {},
   "source": [
    "### try again with our resampled dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7947f7d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-auc:0.91537\n",
      "[1]\tvalidation_0-auc:0.94467\n",
      "[2]\tvalidation_0-auc:0.95298\n",
      "[3]\tvalidation_0-auc:0.97536\n",
      "[4]\tvalidation_0-auc:0.97302\n",
      "[5]\tvalidation_0-auc:0.96835\n",
      "[6]\tvalidation_0-auc:0.97498\n",
      "[7]\tvalidation_0-auc:0.97734\n",
      "[8]\tvalidation_0-auc:0.97602\n",
      "[9]\tvalidation_0-auc:0.97890\n",
      "[10]\tvalidation_0-auc:0.98586\n",
      "[11]\tvalidation_0-auc:0.98516\n",
      "[12]\tvalidation_0-auc:0.98318\n",
      "[13]\tvalidation_0-auc:0.98532\n",
      "[14]\tvalidation_0-auc:0.98668\n",
      "[15]\tvalidation_0-auc:0.98608\n",
      "[16]\tvalidation_0-auc:0.98684\n",
      "[17]\tvalidation_0-auc:0.98738\n",
      "[18]\tvalidation_0-auc:0.98667\n",
      "[19]\tvalidation_0-auc:0.98828\n",
      "[20]\tvalidation_0-auc:0.98735\n",
      "[21]\tvalidation_0-auc:0.98835\n",
      "[22]\tvalidation_0-auc:0.98979\n",
      "[23]\tvalidation_0-auc:0.98882\n",
      "[24]\tvalidation_0-auc:0.98898\n",
      "[25]\tvalidation_0-auc:0.98893\n",
      "[26]\tvalidation_0-auc:0.98900\n",
      "[27]\tvalidation_0-auc:0.98803\n",
      "[28]\tvalidation_0-auc:0.98755\n",
      "[29]\tvalidation_0-auc:0.98791\n",
      "[30]\tvalidation_0-auc:0.98837\n",
      "[31]\tvalidation_0-auc:0.98782\n",
      "[32]\tvalidation_0-auc:0.98806\n",
      "[33]\tvalidation_0-auc:0.98861\n",
      "[34]\tvalidation_0-auc:0.98905\n",
      "[35]\tvalidation_0-auc:0.98954\n",
      "[36]\tvalidation_0-auc:0.99002\n",
      "[37]\tvalidation_0-auc:0.98977\n",
      "[38]\tvalidation_0-auc:0.98928\n",
      "[39]\tvalidation_0-auc:0.98923\n",
      "[40]\tvalidation_0-auc:0.98938\n",
      "[41]\tvalidation_0-auc:0.98927\n",
      "[42]\tvalidation_0-auc:0.99000\n",
      "[43]\tvalidation_0-auc:0.99024\n",
      "[44]\tvalidation_0-auc:0.99003\n",
      "[45]\tvalidation_0-auc:0.99020\n",
      "[46]\tvalidation_0-auc:0.99042\n",
      "[47]\tvalidation_0-auc:0.99071\n",
      "[48]\tvalidation_0-auc:0.99071\n",
      "[49]\tvalidation_0-auc:0.99102\n",
      "[50]\tvalidation_0-auc:0.99136\n",
      "[51]\tvalidation_0-auc:0.99151\n",
      "[52]\tvalidation_0-auc:0.99149\n",
      "[53]\tvalidation_0-auc:0.99133\n",
      "[54]\tvalidation_0-auc:0.99153\n",
      "[55]\tvalidation_0-auc:0.99180\n",
      "[56]\tvalidation_0-auc:0.99191\n",
      "[57]\tvalidation_0-auc:0.99196\n",
      "[58]\tvalidation_0-auc:0.99203\n",
      "[59]\tvalidation_0-auc:0.99213\n",
      "[60]\tvalidation_0-auc:0.99239\n",
      "[61]\tvalidation_0-auc:0.99223\n",
      "[62]\tvalidation_0-auc:0.99240\n",
      "[63]\tvalidation_0-auc:0.99257\n",
      "[64]\tvalidation_0-auc:0.99248\n",
      "[65]\tvalidation_0-auc:0.99246\n",
      "[66]\tvalidation_0-auc:0.99233\n",
      "[67]\tvalidation_0-auc:0.99249\n",
      "[68]\tvalidation_0-auc:0.99255\n",
      "[69]\tvalidation_0-auc:0.99279\n",
      "[70]\tvalidation_0-auc:0.99277\n",
      "[71]\tvalidation_0-auc:0.99265\n",
      "[72]\tvalidation_0-auc:0.99262\n",
      "[73]\tvalidation_0-auc:0.99290\n",
      "[74]\tvalidation_0-auc:0.99313\n",
      "[75]\tvalidation_0-auc:0.99314\n",
      "[76]\tvalidation_0-auc:0.99335\n",
      "[77]\tvalidation_0-auc:0.99337\n",
      "[78]\tvalidation_0-auc:0.99344\n",
      "[79]\tvalidation_0-auc:0.99351\n",
      "[80]\tvalidation_0-auc:0.99353\n",
      "[81]\tvalidation_0-auc:0.99344\n",
      "[82]\tvalidation_0-auc:0.99348\n",
      "[83]\tvalidation_0-auc:0.99349\n",
      "[84]\tvalidation_0-auc:0.99357\n",
      "[85]\tvalidation_0-auc:0.99354\n",
      "[86]\tvalidation_0-auc:0.99357\n",
      "[87]\tvalidation_0-auc:0.99352\n",
      "[88]\tvalidation_0-auc:0.99360\n",
      "[89]\tvalidation_0-auc:0.99378\n",
      "[90]\tvalidation_0-auc:0.99372\n",
      "[91]\tvalidation_0-auc:0.99377\n",
      "[92]\tvalidation_0-auc:0.99370\n",
      "[93]\tvalidation_0-auc:0.99366\n",
      "[94]\tvalidation_0-auc:0.99381\n",
      "[95]\tvalidation_0-auc:0.99382\n",
      "[96]\tvalidation_0-auc:0.99379\n",
      "[97]\tvalidation_0-auc:0.99388\n",
      "[98]\tvalidation_0-auc:0.99386\n",
      "[99]\tvalidation_0-auc:0.99375\n",
      "[100]\tvalidation_0-auc:0.99378\n",
      "[101]\tvalidation_0-auc:0.99397\n",
      "[102]\tvalidation_0-auc:0.99393\n",
      "[103]\tvalidation_0-auc:0.99399\n",
      "[104]\tvalidation_0-auc:0.99420\n",
      "[105]\tvalidation_0-auc:0.99422\n",
      "[106]\tvalidation_0-auc:0.99426\n",
      "[107]\tvalidation_0-auc:0.99422\n",
      "[108]\tvalidation_0-auc:0.99424\n",
      "[109]\tvalidation_0-auc:0.99431\n",
      "[110]\tvalidation_0-auc:0.99434\n",
      "[111]\tvalidation_0-auc:0.99420\n",
      "[112]\tvalidation_0-auc:0.99418\n",
      "[113]\tvalidation_0-auc:0.99426\n",
      "[114]\tvalidation_0-auc:0.99427\n",
      "[115]\tvalidation_0-auc:0.99423\n",
      "[116]\tvalidation_0-auc:0.99435\n",
      "[117]\tvalidation_0-auc:0.99430\n",
      "[118]\tvalidation_0-auc:0.99434\n",
      "[119]\tvalidation_0-auc:0.99435\n",
      "[120]\tvalidation_0-auc:0.99434\n",
      "[121]\tvalidation_0-auc:0.99447\n",
      "[122]\tvalidation_0-auc:0.99455\n",
      "[123]\tvalidation_0-auc:0.99457\n",
      "[124]\tvalidation_0-auc:0.99456\n",
      "[125]\tvalidation_0-auc:0.99459\n",
      "[126]\tvalidation_0-auc:0.99449\n",
      "[127]\tvalidation_0-auc:0.99457\n",
      "[128]\tvalidation_0-auc:0.99461\n",
      "[129]\tvalidation_0-auc:0.99450\n",
      "[130]\tvalidation_0-auc:0.99451\n",
      "[131]\tvalidation_0-auc:0.99445\n",
      "[132]\tvalidation_0-auc:0.99443\n",
      "[133]\tvalidation_0-auc:0.99447\n",
      "[134]\tvalidation_0-auc:0.99451\n",
      "[135]\tvalidation_0-auc:0.99456\n",
      "[136]\tvalidation_0-auc:0.99455\n",
      "[137]\tvalidation_0-auc:0.99446\n",
      "[138]\tvalidation_0-auc:0.99448\n",
      "[139]\tvalidation_0-auc:0.99445\n",
      "[140]\tvalidation_0-auc:0.99443\n",
      "[141]\tvalidation_0-auc:0.99452\n",
      "[142]\tvalidation_0-auc:0.99462\n",
      "[143]\tvalidation_0-auc:0.99463\n",
      "[144]\tvalidation_0-auc:0.99456\n",
      "[145]\tvalidation_0-auc:0.99452\n",
      "[146]\tvalidation_0-auc:0.99450\n",
      "[147]\tvalidation_0-auc:0.99448\n",
      "[148]\tvalidation_0-auc:0.99444\n",
      "[149]\tvalidation_0-auc:0.99436\n",
      "[150]\tvalidation_0-auc:0.99442\n",
      "[151]\tvalidation_0-auc:0.99449\n",
      "[152]\tvalidation_0-auc:0.99448\n",
      "[153]\tvalidation_0-auc:0.99443\n",
      "[154]\tvalidation_0-auc:0.99442\n",
      "[155]\tvalidation_0-auc:0.99438\n",
      "[156]\tvalidation_0-auc:0.99442\n",
      "[157]\tvalidation_0-auc:0.99447\n",
      "[158]\tvalidation_0-auc:0.99446\n",
      "[159]\tvalidation_0-auc:0.99450\n",
      "[160]\tvalidation_0-auc:0.99452\n",
      "[161]\tvalidation_0-auc:0.99451\n",
      "[162]\tvalidation_0-auc:0.99464\n",
      "[163]\tvalidation_0-auc:0.99461\n",
      "[164]\tvalidation_0-auc:0.99467\n",
      "[165]\tvalidation_0-auc:0.99463\n",
      "[166]\tvalidation_0-auc:0.99453\n",
      "[167]\tvalidation_0-auc:0.99453\n",
      "[168]\tvalidation_0-auc:0.99450\n",
      "[169]\tvalidation_0-auc:0.99455\n",
      "[170]\tvalidation_0-auc:0.99454\n",
      "[171]\tvalidation_0-auc:0.99448\n",
      "[172]\tvalidation_0-auc:0.99450\n",
      "[173]\tvalidation_0-auc:0.99450\n",
      "[174]\tvalidation_0-auc:0.99456\n",
      "[175]\tvalidation_0-auc:0.99457\n",
      "[176]\tvalidation_0-auc:0.99452\n",
      "[177]\tvalidation_0-auc:0.99447\n",
      "[178]\tvalidation_0-auc:0.99451\n",
      "[179]\tvalidation_0-auc:0.99456\n",
      "[180]\tvalidation_0-auc:0.99457\n",
      "[181]\tvalidation_0-auc:0.99462\n",
      "[182]\tvalidation_0-auc:0.99465\n",
      "[183]\tvalidation_0-auc:0.99469\n",
      "[184]\tvalidation_0-auc:0.99473\n",
      "[185]\tvalidation_0-auc:0.99476\n",
      "[186]\tvalidation_0-auc:0.99467\n",
      "[187]\tvalidation_0-auc:0.99472\n",
      "[188]\tvalidation_0-auc:0.99476\n",
      "[189]\tvalidation_0-auc:0.99470\n",
      "[190]\tvalidation_0-auc:0.99476\n",
      "[191]\tvalidation_0-auc:0.99484\n",
      "[192]\tvalidation_0-auc:0.99479\n",
      "[193]\tvalidation_0-auc:0.99482\n",
      "[194]\tvalidation_0-auc:0.99480\n",
      "[195]\tvalidation_0-auc:0.99488\n",
      "[196]\tvalidation_0-auc:0.99487\n",
      "[197]\tvalidation_0-auc:0.99492\n",
      "[198]\tvalidation_0-auc:0.99491\n",
      "[199]\tvalidation_0-auc:0.99491\n",
      "[200]\tvalidation_0-auc:0.99492\n",
      "[201]\tvalidation_0-auc:0.99486\n",
      "[202]\tvalidation_0-auc:0.99502\n",
      "[203]\tvalidation_0-auc:0.99502\n",
      "[204]\tvalidation_0-auc:0.99504\n",
      "[205]\tvalidation_0-auc:0.99505\n",
      "[206]\tvalidation_0-auc:0.99510\n",
      "[207]\tvalidation_0-auc:0.99507\n",
      "[208]\tvalidation_0-auc:0.99507\n",
      "[209]\tvalidation_0-auc:0.99504\n",
      "[210]\tvalidation_0-auc:0.99508\n",
      "[211]\tvalidation_0-auc:0.99502\n",
      "[212]\tvalidation_0-auc:0.99509\n",
      "[213]\tvalidation_0-auc:0.99512\n",
      "[214]\tvalidation_0-auc:0.99514\n",
      "[215]\tvalidation_0-auc:0.99524\n",
      "[216]\tvalidation_0-auc:0.99524\n",
      "[217]\tvalidation_0-auc:0.99521\n",
      "[218]\tvalidation_0-auc:0.99524\n",
      "[219]\tvalidation_0-auc:0.99526\n",
      "[220]\tvalidation_0-auc:0.99520\n",
      "[221]\tvalidation_0-auc:0.99523\n",
      "[222]\tvalidation_0-auc:0.99525\n",
      "[223]\tvalidation_0-auc:0.99523\n",
      "[224]\tvalidation_0-auc:0.99522\n",
      "[225]\tvalidation_0-auc:0.99525\n",
      "[226]\tvalidation_0-auc:0.99524\n",
      "[227]\tvalidation_0-auc:0.99516\n",
      "[228]\tvalidation_0-auc:0.99515\n",
      "[229]\tvalidation_0-auc:0.99524\n",
      "[230]\tvalidation_0-auc:0.99524\n",
      "[231]\tvalidation_0-auc:0.99527\n",
      "[232]\tvalidation_0-auc:0.99528\n",
      "[233]\tvalidation_0-auc:0.99525\n",
      "[234]\tvalidation_0-auc:0.99528\n",
      "[235]\tvalidation_0-auc:0.99525\n",
      "[236]\tvalidation_0-auc:0.99529\n",
      "[237]\tvalidation_0-auc:0.99526\n",
      "[238]\tvalidation_0-auc:0.99533\n",
      "[239]\tvalidation_0-auc:0.99530\n",
      "[240]\tvalidation_0-auc:0.99530\n",
      "[241]\tvalidation_0-auc:0.99534\n",
      "[242]\tvalidation_0-auc:0.99536\n",
      "[243]\tvalidation_0-auc:0.99536\n",
      "[244]\tvalidation_0-auc:0.99539\n",
      "[245]\tvalidation_0-auc:0.99542\n",
      "[246]\tvalidation_0-auc:0.99544\n",
      "[247]\tvalidation_0-auc:0.99539\n",
      "[248]\tvalidation_0-auc:0.99538\n",
      "[249]\tvalidation_0-auc:0.99534\n",
      "[250]\tvalidation_0-auc:0.99537\n",
      "[251]\tvalidation_0-auc:0.99537\n",
      "[252]\tvalidation_0-auc:0.99538\n",
      "[253]\tvalidation_0-auc:0.99544\n",
      "[254]\tvalidation_0-auc:0.99547\n",
      "[255]\tvalidation_0-auc:0.99549\n",
      "[256]\tvalidation_0-auc:0.99549\n",
      "[257]\tvalidation_0-auc:0.99550\n",
      "[258]\tvalidation_0-auc:0.99546\n",
      "[259]\tvalidation_0-auc:0.99542\n",
      "[260]\tvalidation_0-auc:0.99541\n",
      "[261]\tvalidation_0-auc:0.99538\n",
      "[262]\tvalidation_0-auc:0.99536\n",
      "[263]\tvalidation_0-auc:0.99542\n",
      "[264]\tvalidation_0-auc:0.99541\n",
      "[265]\tvalidation_0-auc:0.99538\n",
      "[266]\tvalidation_0-auc:0.99535\n",
      "[267]\tvalidation_0-auc:0.99536\n",
      "[268]\tvalidation_0-auc:0.99544\n",
      "[269]\tvalidation_0-auc:0.99545\n",
      "[270]\tvalidation_0-auc:0.99545\n",
      "[271]\tvalidation_0-auc:0.99547\n",
      "[272]\tvalidation_0-auc:0.99551\n",
      "[273]\tvalidation_0-auc:0.99549\n",
      "[274]\tvalidation_0-auc:0.99549\n",
      "[275]\tvalidation_0-auc:0.99547\n",
      "[276]\tvalidation_0-auc:0.99544\n",
      "[277]\tvalidation_0-auc:0.99547\n",
      "[278]\tvalidation_0-auc:0.99554\n",
      "[279]\tvalidation_0-auc:0.99553\n",
      "[280]\tvalidation_0-auc:0.99551\n",
      "[281]\tvalidation_0-auc:0.99549\n",
      "[282]\tvalidation_0-auc:0.99551\n",
      "[283]\tvalidation_0-auc:0.99549\n",
      "[284]\tvalidation_0-auc:0.99556\n",
      "[285]\tvalidation_0-auc:0.99549\n",
      "[286]\tvalidation_0-auc:0.99543\n",
      "[287]\tvalidation_0-auc:0.99549\n",
      "[288]\tvalidation_0-auc:0.99551\n",
      "[289]\tvalidation_0-auc:0.99554\n",
      "[290]\tvalidation_0-auc:0.99550\n",
      "[291]\tvalidation_0-auc:0.99550\n",
      "[292]\tvalidation_0-auc:0.99554\n",
      "[293]\tvalidation_0-auc:0.99552\n",
      "[294]\tvalidation_0-auc:0.99547\n",
      "[295]\tvalidation_0-auc:0.99552\n",
      "[296]\tvalidation_0-auc:0.99548\n",
      "[297]\tvalidation_0-auc:0.99550\n",
      "[298]\tvalidation_0-auc:0.99552\n",
      "[299]\tvalidation_0-auc:0.99555\n",
      "[300]\tvalidation_0-auc:0.99555\n",
      "[301]\tvalidation_0-auc:0.99554\n",
      "[302]\tvalidation_0-auc:0.99553\n",
      "[303]\tvalidation_0-auc:0.99557\n",
      "[304]\tvalidation_0-auc:0.99561\n",
      "[305]\tvalidation_0-auc:0.99564\n",
      "[306]\tvalidation_0-auc:0.99562\n",
      "[307]\tvalidation_0-auc:0.99562\n",
      "[308]\tvalidation_0-auc:0.99567\n",
      "[309]\tvalidation_0-auc:0.99569\n",
      "[310]\tvalidation_0-auc:0.99567\n",
      "[311]\tvalidation_0-auc:0.99571\n",
      "[312]\tvalidation_0-auc:0.99573\n",
      "[313]\tvalidation_0-auc:0.99572\n",
      "[314]\tvalidation_0-auc:0.99567\n",
      "[315]\tvalidation_0-auc:0.99564\n",
      "[316]\tvalidation_0-auc:0.99562\n",
      "[317]\tvalidation_0-auc:0.99560\n",
      "[318]\tvalidation_0-auc:0.99554\n",
      "[319]\tvalidation_0-auc:0.99552\n",
      "[320]\tvalidation_0-auc:0.99551\n",
      "[321]\tvalidation_0-auc:0.99554\n",
      "[322]\tvalidation_0-auc:0.99555\n",
      "[323]\tvalidation_0-auc:0.99558\n",
      "[324]\tvalidation_0-auc:0.99562\n",
      "[325]\tvalidation_0-auc:0.99562\n",
      "[326]\tvalidation_0-auc:0.99563\n",
      "[327]\tvalidation_0-auc:0.99564\n",
      "[328]\tvalidation_0-auc:0.99570\n",
      "[329]\tvalidation_0-auc:0.99569\n",
      "[330]\tvalidation_0-auc:0.99572\n",
      "[331]\tvalidation_0-auc:0.99573\n",
      "[332]\tvalidation_0-auc:0.99577\n",
      "[333]\tvalidation_0-auc:0.99581\n",
      "[334]\tvalidation_0-auc:0.99584\n",
      "[335]\tvalidation_0-auc:0.99587\n",
      "[336]\tvalidation_0-auc:0.99589\n",
      "[337]\tvalidation_0-auc:0.99587\n",
      "[338]\tvalidation_0-auc:0.99588\n",
      "[339]\tvalidation_0-auc:0.99588\n",
      "[340]\tvalidation_0-auc:0.99587\n",
      "[341]\tvalidation_0-auc:0.99587\n",
      "[342]\tvalidation_0-auc:0.99583\n",
      "[343]\tvalidation_0-auc:0.99589\n",
      "[344]\tvalidation_0-auc:0.99586\n",
      "[345]\tvalidation_0-auc:0.99587\n",
      "[346]\tvalidation_0-auc:0.99587\n",
      "[347]\tvalidation_0-auc:0.99588\n",
      "[348]\tvalidation_0-auc:0.99592\n",
      "[349]\tvalidation_0-auc:0.99592\n",
      "[350]\tvalidation_0-auc:0.99590\n",
      "[351]\tvalidation_0-auc:0.99590\n",
      "[352]\tvalidation_0-auc:0.99590\n",
      "[353]\tvalidation_0-auc:0.99588\n",
      "[354]\tvalidation_0-auc:0.99589\n",
      "[355]\tvalidation_0-auc:0.99590\n",
      "[356]\tvalidation_0-auc:0.99583\n",
      "[357]\tvalidation_0-auc:0.99582\n",
      "[358]\tvalidation_0-auc:0.99579\n",
      "[359]\tvalidation_0-auc:0.99582\n",
      "[360]\tvalidation_0-auc:0.99581\n",
      "[361]\tvalidation_0-auc:0.99579\n",
      "[362]\tvalidation_0-auc:0.99578\n",
      "[363]\tvalidation_0-auc:0.99579\n",
      "[364]\tvalidation_0-auc:0.99582\n",
      "[365]\tvalidation_0-auc:0.99588\n",
      "[366]\tvalidation_0-auc:0.99588\n",
      "[367]\tvalidation_0-auc:0.99593\n",
      "[368]\tvalidation_0-auc:0.99596\n",
      "[369]\tvalidation_0-auc:0.99592\n",
      "[370]\tvalidation_0-auc:0.99591\n",
      "[371]\tvalidation_0-auc:0.99594\n",
      "[372]\tvalidation_0-auc:0.99595\n",
      "[373]\tvalidation_0-auc:0.99596\n",
      "[374]\tvalidation_0-auc:0.99596\n",
      "[375]\tvalidation_0-auc:0.99600\n",
      "[376]\tvalidation_0-auc:0.99602\n",
      "[377]\tvalidation_0-auc:0.99603\n",
      "[378]\tvalidation_0-auc:0.99602\n",
      "[379]\tvalidation_0-auc:0.99606\n",
      "[380]\tvalidation_0-auc:0.99606\n",
      "[381]\tvalidation_0-auc:0.99608\n",
      "[382]\tvalidation_0-auc:0.99604\n",
      "[383]\tvalidation_0-auc:0.99607\n",
      "[384]\tvalidation_0-auc:0.99610\n",
      "[385]\tvalidation_0-auc:0.99609\n",
      "[386]\tvalidation_0-auc:0.99618\n",
      "[387]\tvalidation_0-auc:0.99619\n",
      "[388]\tvalidation_0-auc:0.99621\n",
      "[389]\tvalidation_0-auc:0.99621\n",
      "[390]\tvalidation_0-auc:0.99615\n",
      "[391]\tvalidation_0-auc:0.99612\n",
      "[392]\tvalidation_0-auc:0.99613\n",
      "[393]\tvalidation_0-auc:0.99617\n",
      "[394]\tvalidation_0-auc:0.99616\n",
      "[395]\tvalidation_0-auc:0.99620\n",
      "[396]\tvalidation_0-auc:0.99619\n",
      "[397]\tvalidation_0-auc:0.99621\n",
      "[398]\tvalidation_0-auc:0.99621\n",
      "[399]\tvalidation_0-auc:0.99621\n",
      "[400]\tvalidation_0-auc:0.99625\n",
      "[401]\tvalidation_0-auc:0.99626\n",
      "[402]\tvalidation_0-auc:0.99624\n",
      "[403]\tvalidation_0-auc:0.99626\n",
      "[404]\tvalidation_0-auc:0.99628\n",
      "[405]\tvalidation_0-auc:0.99625\n",
      "[406]\tvalidation_0-auc:0.99630\n",
      "[407]\tvalidation_0-auc:0.99631\n",
      "[408]\tvalidation_0-auc:0.99630\n",
      "[409]\tvalidation_0-auc:0.99632\n",
      "[410]\tvalidation_0-auc:0.99631\n",
      "[411]\tvalidation_0-auc:0.99630\n",
      "[412]\tvalidation_0-auc:0.99625\n",
      "[413]\tvalidation_0-auc:0.99627\n",
      "[414]\tvalidation_0-auc:0.99627\n",
      "[415]\tvalidation_0-auc:0.99629\n",
      "[416]\tvalidation_0-auc:0.99633\n",
      "[417]\tvalidation_0-auc:0.99636\n",
      "[418]\tvalidation_0-auc:0.99637\n",
      "[419]\tvalidation_0-auc:0.99639\n",
      "[420]\tvalidation_0-auc:0.99641\n",
      "[421]\tvalidation_0-auc:0.99640\n",
      "[422]\tvalidation_0-auc:0.99641\n",
      "[423]\tvalidation_0-auc:0.99641\n",
      "[424]\tvalidation_0-auc:0.99640\n",
      "[425]\tvalidation_0-auc:0.99647\n",
      "[426]\tvalidation_0-auc:0.99648\n",
      "[427]\tvalidation_0-auc:0.99648\n",
      "[428]\tvalidation_0-auc:0.99648\n",
      "[429]\tvalidation_0-auc:0.99649\n",
      "[430]\tvalidation_0-auc:0.99650\n",
      "[431]\tvalidation_0-auc:0.99650\n",
      "[432]\tvalidation_0-auc:0.99647\n",
      "[433]\tvalidation_0-auc:0.99645\n",
      "[434]\tvalidation_0-auc:0.99647\n",
      "[435]\tvalidation_0-auc:0.99649\n",
      "[436]\tvalidation_0-auc:0.99645\n",
      "[437]\tvalidation_0-auc:0.99646\n",
      "[438]\tvalidation_0-auc:0.99646\n",
      "[439]\tvalidation_0-auc:0.99643\n",
      "[440]\tvalidation_0-auc:0.99644\n",
      "[441]\tvalidation_0-auc:0.99643\n",
      "[442]\tvalidation_0-auc:0.99643\n",
      "[443]\tvalidation_0-auc:0.99644\n",
      "[444]\tvalidation_0-auc:0.99645\n",
      "[445]\tvalidation_0-auc:0.99649\n",
      "[446]\tvalidation_0-auc:0.99649\n",
      "[447]\tvalidation_0-auc:0.99651\n",
      "[448]\tvalidation_0-auc:0.99653\n",
      "[449]\tvalidation_0-auc:0.99655\n",
      "[450]\tvalidation_0-auc:0.99655\n",
      "[451]\tvalidation_0-auc:0.99655\n",
      "[452]\tvalidation_0-auc:0.99655\n",
      "[453]\tvalidation_0-auc:0.99656\n",
      "[454]\tvalidation_0-auc:0.99655\n",
      "[455]\tvalidation_0-auc:0.99655\n",
      "[456]\tvalidation_0-auc:0.99655\n",
      "[457]\tvalidation_0-auc:0.99655\n",
      "[458]\tvalidation_0-auc:0.99656\n",
      "[459]\tvalidation_0-auc:0.99658\n",
      "[460]\tvalidation_0-auc:0.99661\n",
      "[461]\tvalidation_0-auc:0.99659\n",
      "[462]\tvalidation_0-auc:0.99661\n",
      "[463]\tvalidation_0-auc:0.99659\n",
      "[464]\tvalidation_0-auc:0.99663\n",
      "[465]\tvalidation_0-auc:0.99661\n",
      "[466]\tvalidation_0-auc:0.99658\n",
      "[467]\tvalidation_0-auc:0.99660\n",
      "[468]\tvalidation_0-auc:0.99663\n",
      "[469]\tvalidation_0-auc:0.99664\n",
      "[470]\tvalidation_0-auc:0.99662\n",
      "[471]\tvalidation_0-auc:0.99661\n",
      "[472]\tvalidation_0-auc:0.99660\n",
      "[473]\tvalidation_0-auc:0.99656\n",
      "[474]\tvalidation_0-auc:0.99651\n",
      "[475]\tvalidation_0-auc:0.99655\n",
      "[476]\tvalidation_0-auc:0.99652\n",
      "[477]\tvalidation_0-auc:0.99653\n",
      "[478]\tvalidation_0-auc:0.99653\n",
      "[479]\tvalidation_0-auc:0.99652\n",
      "[480]\tvalidation_0-auc:0.99652\n",
      "[481]\tvalidation_0-auc:0.99654\n",
      "[482]\tvalidation_0-auc:0.99655\n",
      "[483]\tvalidation_0-auc:0.99657\n",
      "[484]\tvalidation_0-auc:0.99660\n",
      "[485]\tvalidation_0-auc:0.99663\n",
      "[486]\tvalidation_0-auc:0.99658\n",
      "[487]\tvalidation_0-auc:0.99655\n",
      "[488]\tvalidation_0-auc:0.99658\n",
      "[489]\tvalidation_0-auc:0.99652\n",
      "[490]\tvalidation_0-auc:0.99650\n",
      "[491]\tvalidation_0-auc:0.99650\n",
      "[492]\tvalidation_0-auc:0.99647\n",
      "[493]\tvalidation_0-auc:0.99644\n",
      "[494]\tvalidation_0-auc:0.99643\n",
      "[495]\tvalidation_0-auc:0.99644\n",
      "[496]\tvalidation_0-auc:0.99645\n",
      "[497]\tvalidation_0-auc:0.99645\n",
      "[498]\tvalidation_0-auc:0.99648\n",
      "[499]\tvalidation_0-auc:0.99649\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      1.00      1.00      1660\n",
      "         1.0       1.00      0.91      0.95       166\n",
      "\n",
      "    accuracy                           0.99      1826\n",
      "   macro avg       1.00      0.95      0.97      1826\n",
      "weighted avg       0.99      0.99      0.99      1826\n",
      "\n",
      "AUC Score: 0.9965\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 6. Define the XGBoost model\n",
    "model = XGBClassifier(\n",
    "    n_estimators=500,\n",
    "    objective='binary:logistic',\n",
    "    learning_rate=0.03,\n",
    "    max_depth=6,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    device='cuda',        # Use GPU acceleration\n",
    "    eval_metric='auc'   # Handle imbalance\n",
    ")\n",
    "\n",
    "# 7. Train the model\n",
    "model.fit(\n",
    "    X_param_train,\n",
    "    y_param_train,\n",
    "    eval_set=[(X_param_val, y_param_val)],\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# 8. Predict and evaluate\n",
    "y_pred = model.predict(X_param_val)\n",
    "y_pred_proba = model.predict_proba(X_param_val)[:, 1]\n",
    "\n",
    "# Classification report\n",
    "print(classification_report(y_param_val, y_pred))\n",
    "\n",
    "# AUC score\n",
    "auc = roc_auc_score(y_param_val, y_pred_proba)\n",
    "print(f\"AUC Score: {auc:.4f}\")\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_param_val, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot()\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c7a099",
   "metadata": {},
   "source": [
    "Initial results are pretty good for an untuned model: we've greatly reduced the False Positive Rate over the other models without losing any accuracy. Using the confusion matrix and AUC as my preferred evaluation metrics, let's try some tuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1dc606c5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pos_weight' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m      2\u001b[39m results = []\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m lr \u001b[38;5;129;01min\u001b[39;00m learning_rates:\n\u001b[32m      5\u001b[39m     model = XGBClassifier(\n\u001b[32m      6\u001b[39m         n_estimators=\u001b[32m500\u001b[39m,\n\u001b[32m      7\u001b[39m         objective=\u001b[33m'\u001b[39m\u001b[33mbinary:logistic\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m      8\u001b[39m         learning_rate=lr,\n\u001b[32m      9\u001b[39m         max_depth=\u001b[32m6\u001b[39m,\n\u001b[32m     10\u001b[39m         subsample=\u001b[32m0.8\u001b[39m,\n\u001b[32m     11\u001b[39m         colsample_bytree=\u001b[32m0.8\u001b[39m,\n\u001b[32m     12\u001b[39m         device=\u001b[33m'\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     13\u001b[39m         eval_metric=\u001b[33m'\u001b[39m\u001b[33mauc\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m         scale_pos_weight=\u001b[43mpos_weight\u001b[49m\n\u001b[32m     15\u001b[39m     )\n\u001b[32m     17\u001b[39m     model.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     18\u001b[39m     y_pred_proba = model.predict_proba(X_test)[:, \u001b[32m1\u001b[39m]\n",
      "\u001b[31mNameError\u001b[39m: name 'pos_weight' is not defined"
     ]
    }
   ],
   "source": [
    "learning_rates = [0.01, 0.03, 0.05, 0.1]\n",
    "results = []\n",
    "\n",
    "for lr in learning_rates:\n",
    "    model = XGBClassifier(\n",
    "        n_estimators=500,\n",
    "        objective='binary:logistic',\n",
    "        learning_rate=lr,\n",
    "        max_depth=6,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        device='cuda',\n",
    "        eval_metric='auc',\n",
    "        scale_pos_weight=pos_weight\n",
    "    )\n",
    "    \n",
    "    model.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=False)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    results.append((lr, auc))\n",
    "\n",
    "# Plotting the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot([x[0] for x in results], [x[1] for x in results], marker='o')\n",
    "plt.title('AUC Score vs Learning Rate')\n",
    "plt.xlabel('Learning Rate')\n",
    "plt.ylabel('AUC Score')\n",
    "plt.xticks(learning_rates)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0905ea",
   "metadata": {},
   "source": [
    "The various LR values don't seem to have a meaningful impact on the AUC score, dropping off slightly but then rebounding as we hit the end of the loop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38b1e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depths = [3, 4, 5, 6, 7, 8, 9, 10]\n",
    "results_depth = []\n",
    "\n",
    "for depth in max_depths:\n",
    "    model = XGBClassifier(\n",
    "        n_estimators=500,\n",
    "        objective='binary:logistic',\n",
    "        learning_rate=0.03,\n",
    "        max_depth=depth,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        device='cuda',\n",
    "        eval_metric='auc',\n",
    "        scale_pos_weight=pos_weight\n",
    "    )\n",
    "    \n",
    "    model.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=False)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    results_depth.append((depth, auc))\n",
    "    best_depth = depth if auc == max([x[1] for x in results_depth]) else best_depth\n",
    "\n",
    "\n",
    "best_model = XGBClassifier(\n",
    "    n_estimators=500,\n",
    "    objective='binary:logistic',\n",
    "    learning_rate=0.03,\n",
    "    max_depth=best_depth,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    device='cuda',\n",
    "    eval_metric='auc',\n",
    "    scale_pos_weight=pos_weight\n",
    ")\n",
    "best_model.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=False)\n",
    "y_pred = best_model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Plotting the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot([x[0] for x in results_depth], [x[1] for x in results_depth], marker='o')\n",
    "plt.title('AUC Score vs Max Depth')\n",
    "plt.xlabel('Max Depth')\n",
    "plt.ylabel('AUC Score')\n",
    "plt.xticks(max_depths)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6872951d",
   "metadata": {},
   "source": [
    "Max Depth is a little more interesting. Here the shallower models perform significantly better on the test set, suggesting that the model is overfitting when tree depth is increased. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dcda8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "subsample_rates = [0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "results_subsample = []\n",
    "\n",
    "for rate in subsample_rates:\n",
    "    model = XGBClassifier(\n",
    "        n_estimators=500,\n",
    "        objective='binary:logistic',\n",
    "        learning_rate=0.03,\n",
    "        max_depth=best_depth,\n",
    "        subsample=rate,\n",
    "        colsample_bytree=0.8,\n",
    "        device='cuda',\n",
    "        eval_metric='auc',\n",
    "        scale_pos_weight=pos_weight\n",
    "    )\n",
    "    \n",
    "    model.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=False)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    results_subsample.append((rate, auc))\n",
    "\n",
    "# print classification report for best subsample rate\n",
    "best_subsample_rate = max(results_subsample, key=lambda x: x[1])[0]\n",
    "best_model = XGBClassifier(\n",
    "    n_estimators=500,\n",
    "    objective='binary:logistic',\n",
    "    learning_rate=0.03,\n",
    "    max_depth=best_depth,\n",
    "    subsample=best_subsample_rate,\n",
    "    colsample_bytree=0.8,\n",
    "    device='cuda',\n",
    "    eval_metric='auc',\n",
    "    scale_pos_weight=pos_weight\n",
    ")\n",
    "best_model.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=False)\n",
    "y_pred = best_model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Plotting the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot([x[0] for x in results_subsample], [x[1] for x in results_subsample], marker='o')\n",
    "plt.title('AUC Score vs Subsample Rate')\n",
    "plt.xlabel('Subsample Rate')\n",
    "plt.ylabel('AUC Score')\n",
    "plt.xticks(subsample_rates)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0afc4d",
   "metadata": {},
   "source": [
    "The subsample rate will adjust the Fraction of rows sampled per tree. Values < 1.0 add randomness and reduce overfitting. This is another Regularization technique to confol overfitting. Looks like .8 is the sweetspot here. \n",
    "\n",
    "Now let's put it all together with our best model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226c2df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = XGBClassifier(\n",
    "    n_estimators=500,\n",
    "    objective='binary:logistic',\n",
    "    learning_rate=0.03,\n",
    "    max_depth=best_depth,\n",
    "    subsample=best_subsample_rate,\n",
    "    colsample_bytree=0.8,\n",
    "    device='cuda',\n",
    "    eval_metric='auc',\n",
    "    scale_pos_weight=pos_weight\n",
    ")\n",
    "best_model.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=False)\n",
    "y_pred = best_model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76bc40cb",
   "metadata": {},
   "source": [
    "Next, onto our 2nd classification set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db7929a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/home/pshmo/ai4l_final_project_2/ai4l_final_project/datasets/NFL-Punt-Analytics-Competition/processed_punt_data.csv')\n",
    "# Get boolean columns\n",
    "boolean_columns = df.select_dtypes(include='bool').columns\n",
    "\n",
    "# Convert boolean columns to integers (True -> 1, False -> 0)\n",
    "for col in boolean_columns:\n",
    "    df[col] = df[col].astype(int)\n",
    "\n",
    "# Display the first few rows to verify the conversion\n",
    "print(\"Boolean columns converted to integers:\")\n",
    "print(df[boolean_columns].head())\n",
    "df = df.drop(columns=['season_year', 'gamekey', 'playid', 'gsisid'])\n",
    "\n",
    "# create X and y\n",
    "X = df.drop(columns=['injury'])\n",
    "y = df['injury']\n",
    "\n",
    "# Split into train / test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a9f151",
   "metadata": {},
   "source": [
    "For fun, let's try our best parameters from the previous model and see what we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25af13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# compute class imbalance weight: \n",
    "pos_weight = (len(y) - sum(y)) / sum(y)\n",
    "\n",
    "# fit a basic xgboost classifier\n",
    "model = XGBClassifier(\n",
    "    n_estimators=500,\n",
    "    objective='binary:logistic',\n",
    "    learning_rate=0.03,\n",
    "    max_depth=6,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    device='cuda',\n",
    "    eval_metric='auc',\n",
    "    scale_pos_weight=pos_weight\n",
    ")\n",
    "model.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=True)\n",
    "# Predict and evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "confusion_matrix = confusion_matrix(y_test, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix)\n",
    "disp.plot()\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be327454",
   "metadata": {},
   "source": [
    "Now we're getting somewhere: this is the best classification score I've ever pulled on this dataset. Let's try our tuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35bd44cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = [100, 200, 300, 400, 500]\n",
    "results_n_estimators = []\n",
    "for n in n_estimators:\n",
    "    model = XGBClassifier(\n",
    "        n_estimators=n,\n",
    "        objective='binary:logistic',\n",
    "        learning_rate=0.03,\n",
    "        max_depth=6,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        device='cuda',\n",
    "        eval_metric='auc',\n",
    "        scale_pos_weight=pos_weight\n",
    "    )\n",
    "    \n",
    "    model.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=False)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    results_n_estimators.append((n, auc))\n",
    "# Plotting the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot([x[0] for x in results_n_estimators], [x[1] for x in results_n_estimators], marker='o')\n",
    "plt.title('AUC Score vs Number of Estimators')\n",
    "plt.xlabel('Number of Estimators')\n",
    "plt.ylabel('AUC Score')\n",
    "plt.xticks(n_estimators)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de8011a",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = [0.01, 0.03, 0.05, 0.1]\n",
    "results_lr = []\n",
    "for lr in learning_rates:\n",
    "    model = XGBClassifier(\n",
    "        n_estimators=200,\n",
    "        objective='binary:logistic',\n",
    "        learning_rate=lr,\n",
    "        max_depth=6,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        device='cuda',\n",
    "        eval_metric='auc',\n",
    "        scale_pos_weight=pos_weight\n",
    "    )\n",
    "    \n",
    "    model.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=False)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    results_lr.append((lr, auc))\n",
    "# Plotting the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot([x[0] for x in results_lr], [x[1] for x in results_lr], marker='o')\n",
    "plt.title('AUC Score vs Learning Rate')\n",
    "plt.xlabel('Learning Rate')\n",
    "plt.ylabel('AUC Score')\n",
    "plt.xticks(learning_rates)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325cb7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depths = [3, 4, 5, 6, 7, 8, 9, 10]\n",
    "results_max_depth = []\n",
    "for depth in max_depths:\n",
    "    model = XGBClassifier(\n",
    "        n_estimators=200,\n",
    "        objective='binary:logistic',\n",
    "        learning_rate=0.1,\n",
    "        max_depth=depth,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        device='cuda',\n",
    "        eval_metric='auc',\n",
    "        scale_pos_weight=pos_weight\n",
    "    )\n",
    "    \n",
    "    model.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=False)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    results_max_depth.append((depth, auc))\n",
    "# Plotting the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot([x[0] for x in results_max_depth], [x[1] for x in results_max_depth], marker='o')\n",
    "plt.title('AUC Score vs Max Depth')\n",
    "plt.xlabel('Max Depth')\n",
    "plt.ylabel('AUC Score')\n",
    "plt.xticks(max_depths)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cdbe742",
   "metadata": {},
   "outputs": [],
   "source": [
    "subsample_rates = [0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "results_subsample = []\n",
    "for rate in subsample_rates:\n",
    "    model = XGBClassifier(\n",
    "        n_estimators=200,\n",
    "        objective='binary:logistic',\n",
    "        learning_rate=0.1,\n",
    "        max_depth=7,\n",
    "        subsample=rate,\n",
    "        colsample_bytree=0.8,\n",
    "        device='cuda',\n",
    "        eval_metric='auc',\n",
    "        scale_pos_weight=pos_weight\n",
    "    )\n",
    "    \n",
    "    model.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=False)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    results_subsample.append((rate, auc))\n",
    "# Plotting the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot([x[0] for x in results_subsample], [x[1] for x in results_subsample], marker='o')\n",
    "plt.title('AUC Score vs Subsample Rate')\n",
    "plt.xlabel('Subsample Rate')\n",
    "plt.ylabel('AUC Score')\n",
    "plt.xticks(subsample_rates)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44737134",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "best_model = XGBClassifier(\n",
    "    n_estimators=200,\n",
    "    objective='binary:logistic',\n",
    "    learning_rate=0.1,\n",
    "    max_depth=7,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    device='cuda',\n",
    "    eval_metric='auc',\n",
    "    scale_pos_weight=pos_weight\n",
    ")\n",
    "best_model.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=False)\n",
    "y_pred = best_model.predict(X_test)\n",
    "confusion_matrix = confusion_matrix(y_test, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix)\n",
    "disp.plot()\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(f\"Best Model AUC: {roc_auc_score(y_test, best_model.predict_proba(X_test)[:, 1]):.4f}\")\n",
    "print(f\"Best Model Parameters: {best_model.get_params()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa7def3",
   "metadata": {},
   "source": [
    "Not a lot of movement on the confusion matrix (we've managed to remove the one false positive). All in all we've got a pretty good set of evidence to suggest that gradient boosting is a strong candidate for model selection (I'm still working on getting the full time series represntation working)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372799b5",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids-25.02",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
